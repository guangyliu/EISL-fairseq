nohup: ignoring input
lgy-ngram-1
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 15:27:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='1', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='1', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 15:27:28 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 15:27:28 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 15:27:28 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 15:27:28 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 15:27:28 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 15:27:33 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 15:27:33 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 15:27:33 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 15:27:33 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 15:27:33 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 15:27:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 15:27:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 15:27:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 15:27:36 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 15:27:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 15:27:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 15:27:36 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 15:27:36 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 15:27:37 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 15:27:37 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 15:27:37 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 15:27:37 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 15:27:37 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 15:27:37 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 15:27:38 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 15:27:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 15:27:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 15:27:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 15:27:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 15:27:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 15:27:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 15:27:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2021-11-20 15:28:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2021-11-20 15:28:52.718255: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-11-20 15:28:52.718356: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-11-20 15:28:52.718369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-11-20 15:28:53 | INFO | train_inner | epoch 001:    108 / 2763 bleu_loss=2.546, loss=2.546, ppl=5.84, wps=16824.5, ups=1.5, wpb=11221.4, bsz=364, num_updates=100, lr=6e-06, gnorm=5.226, clip=100, loss_scale=0.5, train_wall=73, gb_free=24, wall=76
2021-11-20 15:29:58 | INFO | train_inner | epoch 001:    208 / 2763 bleu_loss=0.676, loss=0.676, ppl=1.6, wps=17460.9, ups=1.53, wpb=11377, bsz=371.5, num_updates=200, lr=1.2e-05, gnorm=3.38, clip=100, loss_scale=0.5, train_wall=65, gb_free=23.1, wall=142
2021-11-20 15:31:06 | INFO | train_inner | epoch 001:    308 / 2763 bleu_loss=0.161, loss=0.161, ppl=1.12, wps=17597.1, ups=1.48, wpb=11911.8, bsz=351.1, num_updates=300, lr=1.8e-05, gnorm=2.165, clip=100, loss_scale=0.5, train_wall=67, gb_free=20.8, wall=210
2021-11-20 15:32:12 | INFO | train_inner | epoch 001:    408 / 2763 bleu_loss=0.074, loss=0.074, ppl=1.05, wps=17745.5, ups=1.52, wpb=11686.1, bsz=351.2, num_updates=400, lr=2.4e-05, gnorm=0.717, clip=100, loss_scale=0.5, train_wall=66, gb_free=21.6, wall=276
2021-11-20 15:33:15 | INFO | train_inner | epoch 001:    508 / 2763 bleu_loss=0.067, loss=0.067, ppl=1.05, wps=17861.2, ups=1.58, wpb=11278.9, bsz=376, num_updates=500, lr=3e-05, gnorm=0.428, clip=98, loss_scale=0.5, train_wall=63, gb_free=15.1, wall=339
2021-11-20 15:34:19 | INFO | train_inner | epoch 001:    608 / 2763 bleu_loss=0.061, loss=0.061, ppl=1.04, wps=18008.9, ups=1.55, wpb=11607.6, bsz=365.5, num_updates=600, lr=2.96842e-05, gnorm=0.468, clip=89, loss_scale=0.5, train_wall=64, gb_free=13.5, wall=403
2021-11-20 15:35:25 | INFO | train_inner | epoch 001:    708 / 2763 bleu_loss=0.058, loss=0.058, ppl=1.04, wps=17465, ups=1.52, wpb=11518.3, bsz=356.2, num_updates=700, lr=2.93684e-05, gnorm=0.324, clip=84, loss_scale=0.5, train_wall=66, gb_free=15.6, wall=469
2021-11-20 15:36:33 | INFO | train_inner | epoch 001:    808 / 2763 bleu_loss=0.057, loss=0.057, ppl=1.04, wps=17389.2, ups=1.49, wpb=11698.9, bsz=348.1, num_updates=800, lr=2.90526e-05, gnorm=0.506, clip=84, loss_scale=0.5, train_wall=67, gb_free=24.6, wall=537
2021-11-20 15:37:37 | INFO | train_inner | epoch 001:    908 / 2763 bleu_loss=0.058, loss=0.058, ppl=1.04, wps=17801.7, ups=1.54, wpb=11536.2, bsz=370.2, num_updates=900, lr=2.87368e-05, gnorm=0.323, clip=86, loss_scale=0.5, train_wall=64, gb_free=16, wall=601
2021-11-20 15:38:44 | INFO | train_inner | epoch 001:   1008 / 2763 bleu_loss=0.069, loss=0.069, ppl=1.05, wps=17203.3, ups=1.51, wpb=11398.4, bsz=357.8, num_updates=1000, lr=2.84211e-05, gnorm=0.734, clip=88, loss_scale=0.5, train_wall=66, gb_free=16.4, wall=668
2021-11-20 15:38:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2021-11-20 15:39:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2021-11-20 15:39:48 | INFO | train_inner | epoch 001:   1110 / 2763 bleu_loss=0.339, loss=0.339, ppl=1.27, wps=17163.5, ups=1.56, wpb=10980.6, bsz=373.7, num_updates=1100, lr=2.81053e-05, gnorm=10.8, clip=99, loss_scale=0.125, train_wall=64, gb_free=15.7, wall=732
2021-11-20 15:40:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2021-11-20 15:40:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2021-11-20 15:40:53 | INFO | train_inner | epoch 001:   1212 / 2763 bleu_loss=0.779, loss=0.779, ppl=1.72, wps=17488.7, ups=1.53, wpb=11415.5, bsz=365.9, num_updates=1200, lr=2.77895e-05, gnorm=31.763, clip=100, loss_scale=0.0312, train_wall=65, gb_free=16.7, wall=797
2021-11-20 15:41:58 | INFO | train_inner | epoch 001:   1312 / 2763 bleu_loss=1.13, loss=1.13, ppl=2.19, wps=17142.3, ups=1.54, wpb=11103.4, bsz=355.9, num_updates=1300, lr=2.74737e-05, gnorm=22.535, clip=100, loss_scale=0.0312, train_wall=64, gb_free=18.8, wall=862
2021-11-20 15:43:03 | INFO | train_inner | epoch 001:   1412 / 2763 bleu_loss=1.013, loss=1.013, ppl=2.02, wps=18182.8, ups=1.54, wpb=11804.1, bsz=360.8, num_updates=1400, lr=2.71579e-05, gnorm=10.577, clip=100, loss_scale=0.0312, train_wall=65, gb_free=16.1, wall=927
2021-11-20 15:44:08 | INFO | train_inner | epoch 001:   1512 / 2763 bleu_loss=1.19, loss=1.19, ppl=2.28, wps=17434.5, ups=1.53, wpb=11384, bsz=362.7, num_updates=1500, lr=2.68421e-05, gnorm=15.903, clip=99, loss_scale=0.0312, train_wall=65, gb_free=21, wall=992
2021-11-20 15:45:16 | INFO | train_inner | epoch 001:   1612 / 2763 bleu_loss=0.949, loss=0.949, ppl=1.93, wps=17437.2, ups=1.47, wpb=11863.7, bsz=341.4, num_updates=1600, lr=2.65263e-05, gnorm=10.227, clip=100, loss_scale=0.0312, train_wall=68, gb_free=26.5, wall=1060
2021-11-20 15:46:22 | INFO | train_inner | epoch 001:   1712 / 2763 bleu_loss=0.697, loss=0.697, ppl=1.62, wps=17562, ups=1.52, wpb=11529, bsz=363.2, num_updates=1700, lr=2.62105e-05, gnorm=18.45, clip=99, loss_scale=0.0312, train_wall=65, gb_free=21.8, wall=1126
2021-11-20 15:47:29 | INFO | train_inner | epoch 001:   1812 / 2763 bleu_loss=0.615, loss=0.615, ppl=1.53, wps=17801.1, ups=1.48, wpb=11992.3, bsz=348.7, num_updates=1800, lr=2.58947e-05, gnorm=11.846, clip=100, loss_scale=0.0312, train_wall=67, gb_free=18.6, wall=1193
2021-11-20 15:48:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2021-11-20 15:48:35 | INFO | train_inner | epoch 001:   1913 / 2763 bleu_loss=0.767, loss=0.767, ppl=1.7, wps=17257.8, ups=1.52, wpb=11346.2, bsz=366.3, num_updates=1900, lr=2.55789e-05, gnorm=19.767, clip=99, loss_scale=0.0156, train_wall=65, gb_free=18.8, wall=1259
2021-11-20 15:49:39 | INFO | train_inner | epoch 001:   2013 / 2763 bleu_loss=0.948, loss=0.948, ppl=1.93, wps=17091.2, ups=1.56, wpb=10977, bsz=369.6, num_updates=2000, lr=2.52632e-05, gnorm=16.523, clip=100, loss_scale=0.0156, train_wall=64, gb_free=15.8, wall=1323
2021-11-20 15:50:44 | INFO | train_inner | epoch 001:   2113 / 2763 bleu_loss=0.427, loss=0.427, ppl=1.34, wps=17968.5, ups=1.54, wpb=11653.9, bsz=362.8, num_updates=2100, lr=2.49474e-05, gnorm=8.865, clip=100, loss_scale=0.0156, train_wall=65, gb_free=18.1, wall=1388
2021-11-20 15:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125
2021-11-20 15:51:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00390625
2021-11-20 15:51:51 | INFO | train_inner | epoch 001:   2215 / 2763 bleu_loss=0.804, loss=0.804, ppl=1.75, wps=17568.4, ups=1.49, wpb=11814, bsz=352.8, num_updates=2200, lr=2.46316e-05, gnorm=28.157, clip=83, loss_scale=0.0039, train_wall=67, gb_free=16.4, wall=1455
2021-11-20 15:52:56 | INFO | train_inner | epoch 001:   2315 / 2763 bleu_loss=3.194, loss=3.194, ppl=9.15, wps=17556, ups=1.53, wpb=11469.6, bsz=362.9, num_updates=2300, lr=2.43158e-05, gnorm=15.629, clip=99, loss_scale=0.0039, train_wall=65, gb_free=19.1, wall=1520
2021-11-20 15:54:02 | INFO | train_inner | epoch 001:   2415 / 2763 bleu_loss=1.924, loss=1.924, ppl=3.8, wps=17795.7, ups=1.52, wpb=11673.3, bsz=354.7, num_updates=2400, lr=2.4e-05, gnorm=6.513, clip=70, loss_scale=0.0039, train_wall=65, gb_free=22.7, wall=1586
2021-11-20 15:55:03 | INFO | train_inner | epoch 001:   2515 / 2763 bleu_loss=0.002, loss=0.002, ppl=1, wps=18673.1, ups=1.63, wpb=11433.3, bsz=377.7, num_updates=2500, lr=2.36842e-05, gnorm=0.081, clip=13, loss_scale=0.0039, train_wall=61, gb_free=24.1, wall=1647
2021-11-20 15:56:08 | INFO | train_inner | epoch 001:   2615 / 2763 bleu_loss=0.001, loss=0.001, ppl=1, wps=17854.2, ups=1.55, wpb=11527.7, bsz=367.6, num_updates=2600, lr=2.33684e-05, gnorm=0.219, clip=7, loss_scale=0.0039, train_wall=64, gb_free=18.9, wall=1712
2021-11-20 15:57:12 | INFO | train_inner | epoch 001:   2715 / 2763 bleu_loss=0.001, loss=0.001, ppl=1, wps=18194.5, ups=1.56, wpb=11652.3, bsz=364.6, num_updates=2700, lr=2.30526e-05, gnorm=0.019, clip=6, loss_scale=0.0039, train_wall=64, gb_free=19, wall=1776
Train_lgy TIME: -1804.405
2021-11-20 15:57:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-11-20 15:59:01 | INFO | valid | epoch 001 | valid on 'valid' subset | bleu_loss 0 | loss 0 | ppl 1 | bleu 0 | wps 1074.9 | wpb 2579.4 | bsz 91 | num_updates 2748
valid_lgy time: 79.422
2021-11-20 15:59:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2748 updates
2021-11-20 15:59:04 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt
2021-11-20 15:59:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt (epoch 1 @ 2748 updates, score 0.0) (writing took 4.380213551223278 seconds)
2021-11-20 15:59:06 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-11-20 15:59:06 | INFO | train | epoch 001 | bleu_loss 0.674 | loss 0.674 | ppl 1.6 | wps 16818.9 | ups 1.46 | wpb 11512.6 | bsz 361.8 | num_updates 2748 | lr 2.29011e-05 | gnorm 8.812 | clip 83.8 | loss_scale 0.0039 | train_wall 1794 | gb_free 23.2 | wall 1890
2021-11-20 15:59:06 | INFO | fairseq_cli.train | done training in 1888.2 seconds
lgy-ngram-1
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 15:59:10 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='1', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='1', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 15:59:10 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 15:59:10 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 15:59:10 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 15:59:10 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 15:59:10 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 15:59:15 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 15:59:15 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 15:59:15 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 15:59:15 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 15:59:15 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 15:59:18 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 15:59:18 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 15:59:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 15:59:18 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 15:59:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 15:59:18 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 15:59:18 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 15:59:18 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 15:59:20 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 15:59:20 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 15:59:20 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 15:59:20 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 15:59:20 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 15:59:20 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 15:59:20 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 15:59:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 15:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 15:59:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 15:59:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 15:59:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 15:59:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 15:59:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2021-11-20 16:00:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2021-11-20 16:00:33.090825: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-11-20 16:00:33.090928: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-11-20 16:00:33.090942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-11-20 16:00:33 | INFO | train_inner | epoch 001:    108 / 2763 bleu_loss=2.546, loss=2.546, ppl=5.84, wps=16990.2, ups=1.51, wpb=11221.4, bsz=364, num_updates=100, lr=6e-06, gnorm=5.222, clip=100, loss_scale=0.5, train_wall=71, gb_free=24, wall=73
2021-11-20 16:01:38 | INFO | train_inner | epoch 001:    208 / 2763 bleu_loss=0.672, loss=0.672, ppl=1.59, wps=17588.1, ups=1.55, wpb=11377, bsz=371.5, num_updates=200, lr=1.2e-05, gnorm=3.411, clip=100, loss_scale=0.5, train_wall=64, gb_free=23.1, wall=140
2021-11-20 16:02:46 | INFO | train_inner | epoch 001:    308 / 2763 bleu_loss=0.139, loss=0.139, ppl=1.1, wps=17653.5, ups=1.48, wpb=11911.8, bsz=351.1, num_updates=300, lr=1.8e-05, gnorm=1.893, clip=100, loss_scale=0.5, train_wall=67, gb_free=20.8, wall=207
2021-11-20 16:03:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2021-11-20 16:03:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2021-11-20 16:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2021-11-20 16:03:53 | INFO | train_inner | epoch 001:    411 / 2763 bleu_loss=0.141, loss=0.141, ppl=1.1, wps=17161.5, ups=1.49, wpb=11554.7, bsz=356.4, num_updates=400, lr=2.4e-05, gnorm=5.736, clip=100, loss_scale=0.0625, train_wall=67, gb_free=22.1, wall=274
2021-11-20 16:04:55 | INFO | train_inner | epoch 001:    511 / 2763 bleu_loss=0.074, loss=0.074, ppl=1.05, wps=18070.1, ups=1.6, wpb=11299.2, bsz=372.9, num_updates=500, lr=3e-05, gnorm=1.098, clip=100, loss_scale=0.0625, train_wall=62, gb_free=17, wall=337
2021-11-20 16:05:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2021-11-20 16:05:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2021-11-20 16:06:01 | INFO | train_inner | epoch 001:    613 / 2763 bleu_loss=2.642, loss=2.642, ppl=6.24, wps=17695, ups=1.53, wpb=11546.4, bsz=368.2, num_updates=600, lr=2.96842e-05, gnorm=98.667, clip=100, loss_scale=0.0156, train_wall=65, gb_free=25.4, wall=402
2021-11-20 16:07:06 | INFO | train_inner | epoch 001:    713 / 2763 bleu_loss=0.214, loss=0.214, ppl=1.16, wps=17407.9, ups=1.52, wpb=11443.5, bsz=354, num_updates=700, lr=2.93684e-05, gnorm=26.306, clip=100, loss_scale=0.0156, train_wall=65, gb_free=26, wall=468
2021-11-20 16:07:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125
2021-11-20 16:08:14 | INFO | train_inner | epoch 001:    814 / 2763 bleu_loss=1.139, loss=1.139, ppl=2.2, wps=17557.2, ups=1.49, wpb=11809.8, bsz=351, num_updates=800, lr=2.90526e-05, gnorm=41.08, clip=100, loss_scale=0.0078, train_wall=67, gb_free=15.9, wall=535
2021-11-20 16:09:18 | INFO | train_inner | epoch 001:    914 / 2763 bleu_loss=0.79, loss=0.79, ppl=1.73, wps=17846, ups=1.55, wpb=11479.5, bsz=370.2, num_updates=900, lr=2.87368e-05, gnorm=21.042, clip=100, loss_scale=0.0078, train_wall=64, gb_free=19.4, wall=600
2021-11-20 16:10:23 | INFO | train_inner | epoch 001:   1014 / 2763 bleu_loss=0.658, loss=0.658, ppl=1.58, wps=17280.4, ups=1.53, wpb=11305.4, bsz=359.3, num_updates=1000, lr=2.84211e-05, gnorm=19.21, clip=100, loss_scale=0.0078, train_wall=65, gb_free=19.6, wall=665
2021-11-20 16:11:26 | INFO | train_inner | epoch 001:   1114 / 2763 bleu_loss=0.676, loss=0.676, ppl=1.6, wps=17576.3, ups=1.59, wpb=11053.6, bsz=370.7, num_updates=1100, lr=2.81053e-05, gnorm=8.897, clip=100, loss_scale=0.0078, train_wall=63, gb_free=18.5, wall=728
2021-11-20 16:12:30 | INFO | train_inner | epoch 001:   1214 / 2763 bleu_loss=0.865, loss=0.865, ppl=1.82, wps=17850.9, ups=1.58, wpb=11303.8, bsz=367.6, num_updates=1200, lr=2.77895e-05, gnorm=15.591, clip=100, loss_scale=0.0078, train_wall=63, gb_free=23.8, wall=791
2021-11-20 16:13:35 | INFO | train_inner | epoch 001:   1314 / 2763 bleu_loss=0.808, loss=0.808, ppl=1.75, wps=17350, ups=1.54, wpb=11260.3, bsz=354.6, num_updates=1300, lr=2.74737e-05, gnorm=18.287, clip=100, loss_scale=0.0078, train_wall=65, gb_free=18.8, wall=856
2021-11-20 16:14:39 | INFO | train_inner | epoch 001:   1414 / 2763 bleu_loss=0.523, loss=0.523, ppl=1.44, wps=18366.9, ups=1.56, wpb=11777.8, bsz=363, num_updates=1400, lr=2.71579e-05, gnorm=17.743, clip=100, loss_scale=0.0078, train_wall=64, gb_free=20.9, wall=920
2021-11-20 16:15:43 | INFO | train_inner | epoch 001:   1514 / 2763 bleu_loss=0.552, loss=0.552, ppl=1.47, wps=17577.6, ups=1.55, wpb=11319.9, bsz=361.3, num_updates=1500, lr=2.68421e-05, gnorm=12.3, clip=100, loss_scale=0.0078, train_wall=64, gb_free=15.7, wall=985
2021-11-20 16:16:50 | INFO | train_inner | epoch 001:   1614 / 2763 bleu_loss=0.404, loss=0.404, ppl=1.32, wps=17594.8, ups=1.49, wpb=11845, bsz=341.3, num_updates=1600, lr=2.65263e-05, gnorm=12.046, clip=100, loss_scale=0.0078, train_wall=67, gb_free=20.4, wall=1052
2021-11-20 16:17:56 | INFO | train_inner | epoch 001:   1714 / 2763 bleu_loss=0.278, loss=0.278, ppl=1.21, wps=17696.3, ups=1.53, wpb=11555.7, bsz=363.7, num_updates=1700, lr=2.62105e-05, gnorm=17.682, clip=100, loss_scale=0.0078, train_wall=65, gb_free=15.5, wall=1117
2021-11-20 16:19:02 | INFO | train_inner | epoch 001:   1814 / 2763 bleu_loss=0.176, loss=0.176, ppl=1.13, wps=17968.7, ups=1.5, wpb=11963.3, bsz=350.8, num_updates=1800, lr=2.58947e-05, gnorm=14.448, clip=98, loss_scale=0.0078, train_wall=66, gb_free=22.2, wall=1184
2021-11-20 16:20:07 | INFO | train_inner | epoch 001:   1914 / 2763 bleu_loss=0.159, loss=0.159, ppl=1.12, wps=17752.1, ups=1.56, wpb=11410.9, bsz=362.1, num_updates=1900, lr=2.55789e-05, gnorm=7.445, clip=95, loss_scale=0.0078, train_wall=64, gb_free=22, wall=1248
2021-11-20 16:21:11 | INFO | train_inner | epoch 001:   2014 / 2763 bleu_loss=0.404, loss=0.404, ppl=1.32, wps=17193.6, ups=1.56, wpb=11017.7, bsz=369, num_updates=2000, lr=2.52632e-05, gnorm=16.54, clip=100, loss_scale=0.0078, train_wall=64, gb_free=15.7, wall=1312
2021-11-20 16:22:15 | INFO | train_inner | epoch 001:   2114 / 2763 bleu_loss=0.326, loss=0.326, ppl=1.25, wps=18119.6, ups=1.55, wpb=11667.4, bsz=363.2, num_updates=2100, lr=2.49474e-05, gnorm=11.422, clip=99, loss_scale=0.0078, train_wall=64, gb_free=21.3, wall=1377
2021-11-20 16:23:20 | INFO | train_inner | epoch 001:   2214 / 2763 bleu_loss=0.419, loss=0.419, ppl=1.34, wps=18037.7, ups=1.54, wpb=11750.1, bsz=354.2, num_updates=2200, lr=2.46316e-05, gnorm=13.039, clip=100, loss_scale=0.0078, train_wall=65, gb_free=16.1, wall=1442
2021-11-20 16:24:25 | INFO | train_inner | epoch 001:   2314 / 2763 bleu_loss=0.519, loss=0.519, ppl=1.43, wps=17717, ups=1.54, wpb=11494, bsz=362.5, num_updates=2300, lr=2.43158e-05, gnorm=16.388, clip=98, loss_scale=0.0078, train_wall=65, gb_free=17, wall=1507
2021-11-20 16:25:30 | INFO | train_inner | epoch 001:   2414 / 2763 bleu_loss=0.204, loss=0.204, ppl=1.15, wps=17843.4, ups=1.53, wpb=11650.2, bsz=353.4, num_updates=2400, lr=2.4e-05, gnorm=7.913, clip=99, loss_scale=0.0078, train_wall=65, gb_free=19.6, wall=1572
2021-11-20 16:26:32 | INFO | train_inner | epoch 001:   2514 / 2763 bleu_loss=0.814, loss=0.814, ppl=1.76, wps=18596.7, ups=1.62, wpb=11453, bsz=377.6, num_updates=2500, lr=2.36842e-05, gnorm=20.408, clip=100, loss_scale=0.0078, train_wall=61, gb_free=21.2, wall=1634
2021-11-20 16:27:36 | INFO | train_inner | epoch 001:   2614 / 2763 bleu_loss=0.514, loss=0.514, ppl=1.43, wps=18019, ups=1.57, wpb=11495.7, bsz=369, num_updates=2600, lr=2.33684e-05, gnorm=15.645, clip=100, loss_scale=0.0078, train_wall=63, gb_free=18.1, wall=1697
2021-11-20 16:28:40 | INFO | train_inner | epoch 001:   2714 / 2763 bleu_loss=0.601, loss=0.601, ppl=1.52, wps=18221.5, ups=1.57, wpb=11642.7, bsz=366.7, num_updates=2700, lr=2.30526e-05, gnorm=21.945, clip=100, loss_scale=0.0078, train_wall=64, gb_free=16.5, wall=1761
Train_lgy TIME: -1790.743
2021-11-20 16:29:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-11-20 16:29:27 | INFO | valid | epoch 001 | valid on 'valid' subset | bleu_loss 0.521 | loss 0.521 | ppl 1.43 | bleu 0 | wps 5471.7 | wpb 2579.4 | bsz 91 | num_updates 2749
valid_lgy time: 16.401
2021-11-20 16:29:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2749 updates
2021-11-20 16:29:33 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt
2021-11-20 16:29:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt (epoch 1 @ 2749 updates, score 0.0) (writing took 11.574738007038832 seconds)
2021-11-20 16:29:39 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-11-20 16:29:39 | INFO | train | epoch 001 | bleu_loss 0.633 | loss 0.633 | ppl 1.55 | wps 17445.2 | ups 1.52 | wpb 11504.8 | bsz 362 | num_updates 2749 | lr 2.28979e-05 | gnorm 17.552 | clip 99.6 | loss_scale 0.0078 | train_wall 1780 | gb_free 23.2 | wall 1820
2021-11-20 16:29:39 | INFO | fairseq_cli.train | done training in 1818.8 seconds
lgy-ngram-1
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 16:29:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='1', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='1', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 16:29:43 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 16:29:43 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 16:29:43 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 16:29:43 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 16:29:43 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 16:29:48 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 16:29:48 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 16:29:48 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 16:29:48 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 16:29:48 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 16:29:51 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 16:29:51 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 16:29:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 16:29:51 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 16:29:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 16:29:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 16:29:51 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 16:29:51 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 16:29:52 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 16:29:52 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 16:29:52 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 16:29:52 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 16:29:52 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 16:29:53 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 16:29:53 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 16:29:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 16:29:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 16:29:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 16:29:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 16:29:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 16:29:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 16:30:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2021-11-20 16:30:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2021-11-20 16:31:06.532058: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-11-20 16:31:06.532163: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-11-20 16:31:06.532178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-11-20 16:31:07 | INFO | train_inner | epoch 001:    108 / 2763 bleu_loss=2.547, loss=2.547, ppl=5.84, wps=16869.6, ups=1.5, wpb=11221.4, bsz=364, num_updates=100, lr=6e-06, gnorm=5.225, clip=100, loss_scale=0.5, train_wall=72, gb_free=24, wall=74
2021-11-20 16:32:12 | INFO | train_inner | epoch 001:    208 / 2763 bleu_loss=0.681, loss=0.681, ppl=1.6, wps=17458.7, ups=1.53, wpb=11377, bsz=371.5, num_updates=200, lr=1.2e-05, gnorm=3.467, clip=100, loss_scale=0.5, train_wall=65, gb_free=23.1, wall=141
2021-11-20 16:33:20 | INFO | train_inner | epoch 001:    308 / 2763 bleu_loss=0.155, loss=0.155, ppl=1.11, wps=17489.2, ups=1.47, wpb=11911.8, bsz=351.1, num_updates=300, lr=1.8e-05, gnorm=1.986, clip=100, loss_scale=0.5, train_wall=68, gb_free=20.8, wall=209
2021-11-20 16:34:26 | INFO | train_inner | epoch 001:    408 / 2763 bleu_loss=0.076, loss=0.076, ppl=1.05, wps=17649.8, ups=1.51, wpb=11686.1, bsz=351.2, num_updates=400, lr=2.4e-05, gnorm=0.863, clip=100, loss_scale=0.5, train_wall=66, gb_free=21.6, wall=275
2021-11-20 16:35:30 | INFO | train_inner | epoch 001:    508 / 2763 bleu_loss=0.07, loss=0.07, ppl=1.05, wps=17757.6, ups=1.57, wpb=11278.9, bsz=376, num_updates=500, lr=3e-05, gnorm=0.586, clip=96, loss_scale=0.5, train_wall=63, gb_free=15.1, wall=339
2021-11-20 16:36:34 | INFO | train_inner | epoch 001:    608 / 2763 bleu_loss=0.061, loss=0.061, ppl=1.04, wps=17984.1, ups=1.55, wpb=11607.6, bsz=365.5, num_updates=600, lr=2.96842e-05, gnorm=0.405, clip=94, loss_scale=0.5, train_wall=64, gb_free=13.5, wall=403
2021-11-20 16:37:41 | INFO | train_inner | epoch 001:    708 / 2763 bleu_loss=0.058, loss=0.058, ppl=1.04, wps=17408.2, ups=1.51, wpb=11518.3, bsz=356.2, num_updates=700, lr=2.93684e-05, gnorm=0.348, clip=88, loss_scale=0.5, train_wall=66, gb_free=15.6, wall=469
2021-11-20 16:38:48 | INFO | train_inner | epoch 001:    808 / 2763 bleu_loss=0.055, loss=0.055, ppl=1.04, wps=17450.7, ups=1.49, wpb=11698.9, bsz=348.1, num_updates=800, lr=2.90526e-05, gnorm=0.419, clip=83, loss_scale=0.5, train_wall=67, gb_free=24.6, wall=537
2021-11-20 16:39:53 | INFO | train_inner | epoch 001:    908 / 2763 bleu_loss=0.059, loss=0.059, ppl=1.04, wps=17710.8, ups=1.54, wpb=11536.2, bsz=370.2, num_updates=900, lr=2.87368e-05, gnorm=0.39, clip=84, loss_scale=0.5, train_wall=65, gb_free=16, wall=602
2021-11-20 16:40:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2021-11-20 16:40:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2021-11-20 16:40:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2021-11-20 16:40:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2021-11-20 16:41:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2021-11-20 16:41:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125
2021-11-20 16:41:02 | INFO | train_inner | epoch 001:   1014 / 2763 bleu_loss=0.781, loss=0.781, ppl=1.72, wps=16100.8, ups=1.45, wpb=11141.1, bsz=362.9, num_updates=1000, lr=2.84211e-05, gnorm=11.483, clip=92, loss_scale=0.0078, train_wall=69, gb_free=19.3, wall=671
2021-11-20 16:42:05 | INFO | train_inner | epoch 001:   1114 / 2763 bleu_loss=0.474, loss=0.474, ppl=1.39, wps=17495.3, ups=1.58, wpb=11053.6, bsz=370.7, num_updates=1100, lr=2.81053e-05, gnorm=14.159, clip=50, loss_scale=0.0078, train_wall=63, gb_free=18.5, wall=734
2021-11-20 16:43:08 | INFO | train_inner | epoch 001:   1214 / 2763 bleu_loss=0.001, loss=0.001, ppl=1, wps=17907.5, ups=1.58, wpb=11303.8, bsz=367.6, num_updates=1200, lr=2.77895e-05, gnorm=0.034, clip=2, loss_scale=0.0078, train_wall=63, gb_free=23.8, wall=797
2021-11-20 16:44:13 | INFO | train_inner | epoch 001:   1314 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17488.8, ups=1.55, wpb=11260.3, bsz=354.6, num_updates=1300, lr=2.74737e-05, gnorm=0.013, clip=1, loss_scale=0.0078, train_wall=64, gb_free=18.8, wall=862
2021-11-20 16:45:17 | INFO | train_inner | epoch 001:   1414 / 2763 bleu_loss=0, loss=0, ppl=1, wps=18404, ups=1.56, wpb=11777.8, bsz=363, num_updates=1400, lr=2.71579e-05, gnorm=0.089, clip=2, loss_scale=0.0078, train_wall=64, gb_free=20.9, wall=926
2021-11-20 16:46:22 | INFO | train_inner | epoch 001:   1514 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17434.1, ups=1.54, wpb=11319.9, bsz=361.3, num_updates=1500, lr=2.68421e-05, gnorm=0.026, clip=1, loss_scale=0.0078, train_wall=65, gb_free=15.7, wall=990
2021-11-20 16:47:29 | INFO | train_inner | epoch 001:   1614 / 2763 bleu_loss=0.001, loss=0.001, ppl=1, wps=17505.2, ups=1.48, wpb=11845, bsz=341.3, num_updates=1600, lr=2.65263e-05, gnorm=0.04, clip=5, loss_scale=0.0078, train_wall=67, gb_free=20.4, wall=1058
2021-11-20 16:48:35 | INFO | train_inner | epoch 001:   1714 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17677.4, ups=1.53, wpb=11555.7, bsz=363.7, num_updates=1700, lr=2.62105e-05, gnorm=0.014, clip=3, loss_scale=0.0078, train_wall=65, gb_free=15.5, wall=1124
2021-11-20 16:49:41 | INFO | train_inner | epoch 001:   1814 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17898.9, ups=1.5, wpb=11963.3, bsz=350.8, num_updates=1800, lr=2.58947e-05, gnorm=0.011, clip=1, loss_scale=0.0078, train_wall=66, gb_free=22.2, wall=1190
2021-11-20 16:50:45 | INFO | train_inner | epoch 001:   1914 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17830.7, ups=1.56, wpb=11410.9, bsz=362.1, num_updates=1900, lr=2.55789e-05, gnorm=0.004, clip=0, loss_scale=0.0078, train_wall=64, gb_free=22, wall=1254
2021-11-20 16:51:49 | INFO | train_inner | epoch 001:   2014 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17225, ups=1.56, wpb=11017.7, bsz=369, num_updates=2000, lr=2.52632e-05, gnorm=0.008, clip=1, loss_scale=0.0078, train_wall=64, gb_free=15.7, wall=1318
2021-11-20 16:52:53 | INFO | train_inner | epoch 001:   2114 / 2763 bleu_loss=0, loss=0, ppl=1, wps=18258.4, ups=1.56, wpb=11667.4, bsz=363.2, num_updates=2100, lr=2.49474e-05, gnorm=0.011, clip=1, loss_scale=0.0078, train_wall=64, gb_free=21.3, wall=1382
2021-11-20 16:53:59 | INFO | train_inner | epoch 001:   2214 / 2763 bleu_loss=0, loss=0, ppl=1, wps=18009, ups=1.53, wpb=11750.1, bsz=354.2, num_updates=2200, lr=2.46316e-05, gnorm=0.003, clip=0, loss_scale=0.0078, train_wall=65, gb_free=16.1, wall=1447
2021-11-20 16:55:03 | INFO | train_inner | epoch 001:   2314 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17749.3, ups=1.54, wpb=11494, bsz=362.5, num_updates=2300, lr=2.43158e-05, gnorm=0.006, clip=1, loss_scale=0.0078, train_wall=64, gb_free=17, wall=1512
2021-11-20 16:56:09 | INFO | train_inner | epoch 001:   2414 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17780.3, ups=1.53, wpb=11650.2, bsz=353.4, num_updates=2400, lr=2.4e-05, gnorm=0.007, clip=2, loss_scale=0.0078, train_wall=65, gb_free=19.6, wall=1578
2021-11-20 16:57:10 | INFO | train_inner | epoch 001:   2514 / 2763 bleu_loss=0, loss=0, ppl=1, wps=18595.7, ups=1.62, wpb=11453, bsz=377.6, num_updates=2500, lr=2.36842e-05, gnorm=0.01, clip=3, loss_scale=0.0078, train_wall=61, gb_free=21.2, wall=1639
2021-11-20 16:58:15 | INFO | train_inner | epoch 001:   2614 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17928.9, ups=1.56, wpb=11495.7, bsz=369, num_updates=2600, lr=2.33684e-05, gnorm=0.007, clip=1, loss_scale=0.0078, train_wall=64, gb_free=18.1, wall=1703
2021-11-20 16:59:18 | INFO | train_inner | epoch 001:   2714 / 2763 bleu_loss=0, loss=0, ppl=1, wps=18266.6, ups=1.57, wpb=11642.7, bsz=366.7, num_updates=2700, lr=2.30526e-05, gnorm=0.005, clip=0, loss_scale=0.0078, train_wall=63, gb_free=16.5, wall=1767
Train_lgy TIME: -1796.445
2021-11-20 16:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-11-20 17:01:09 | INFO | valid | epoch 001 | valid on 'valid' subset | bleu_loss 0 | loss 0 | ppl 1 | bleu 0 | wps 1068.6 | wpb 2579.4 | bsz 91 | num_updates 2749
valid_lgy time: 79.999
2021-11-20 17:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2749 updates
2021-11-20 17:01:15 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt
2021-11-20 17:01:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt (epoch 1 @ 2749 updates, score 0.0) (writing took 11.50976612791419 seconds)
2021-11-20 17:01:21 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-11-20 17:01:21 | INFO | train | epoch 001 | bleu_loss 0.179 | loss 0.179 | ppl 1.13 | wps 16806.1 | ups 1.46 | wpb 11505.8 | bsz 361.9 | num_updates 2749 | lr 2.28979e-05 | gnorm 1.441 | clip 36.8 | loss_scale 0.0078 | train_wall 1785 | gb_free 23.2 | wall 1890
2021-11-20 17:01:21 | INFO | fairseq_cli.train | done training in 1888.0 seconds
lgy-ngram-1
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 17:01:25 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='1', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='1', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 17:01:25 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 17:01:25 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 17:01:25 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 17:01:25 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 17:01:25 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 17:01:30 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 17:01:30 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 17:01:30 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 17:01:30 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 17:01:30 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 17:01:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 17:01:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 17:01:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 17:01:33 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 17:01:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 17:01:33 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 17:01:33 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 17:01:33 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 17:01:34 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 17:01:34 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 17:01:34 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 17:01:35 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 17:01:35 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 17:01:35 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 17:01:35 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 17:01:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 17:01:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 17:01:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 17:01:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 17:01:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 17:01:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 17:01:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2021-11-20 17:02:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2021-11-20 17:02:47.789980: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-11-20 17:02:47.790087: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-11-20 17:02:47.790101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-11-20 17:02:48 | INFO | train_inner | epoch 001:    108 / 2763 bleu_loss=2.547, loss=2.547, ppl=5.84, wps=17039.5, ups=1.52, wpb=11221.4, bsz=364, num_updates=100, lr=6e-06, gnorm=5.235, clip=100, loss_scale=0.5, train_wall=71, gb_free=24, wall=73
2021-11-20 17:03:53 | INFO | train_inner | epoch 001:    208 / 2763 bleu_loss=0.682, loss=0.682, ppl=1.6, wps=17504.2, ups=1.54, wpb=11377, bsz=371.5, num_updates=200, lr=1.2e-05, gnorm=3.532, clip=100, loss_scale=0.5, train_wall=65, gb_free=23.1, wall=140
2021-11-20 17:05:01 | INFO | train_inner | epoch 001:    308 / 2763 bleu_loss=0.147, loss=0.147, ppl=1.11, wps=17527.2, ups=1.47, wpb=11911.8, bsz=351.1, num_updates=300, lr=1.8e-05, gnorm=1.968, clip=100, loss_scale=0.5, train_wall=68, gb_free=20.8, wall=208
2021-11-20 17:06:07 | INFO | train_inner | epoch 001:    408 / 2763 bleu_loss=0.073, loss=0.073, ppl=1.05, wps=17721.1, ups=1.52, wpb=11686.1, bsz=351.2, num_updates=400, lr=2.4e-05, gnorm=0.762, clip=100, loss_scale=0.5, train_wall=66, gb_free=21.6, wall=274
2021-11-20 17:07:10 | INFO | train_inner | epoch 001:    508 / 2763 bleu_loss=0.076, loss=0.076, ppl=1.05, wps=17962.8, ups=1.59, wpb=11278.9, bsz=376, num_updates=500, lr=3e-05, gnorm=1.372, clip=100, loss_scale=0.5, train_wall=62, gb_free=15.1, wall=337
2021-11-20 17:08:15 | INFO | train_inner | epoch 001:    608 / 2763 bleu_loss=0.067, loss=0.067, ppl=1.05, wps=17905.7, ups=1.54, wpb=11607.6, bsz=365.5, num_updates=600, lr=2.96842e-05, gnorm=0.501, clip=94, loss_scale=0.5, train_wall=64, gb_free=13.5, wall=401
2021-11-20 17:09:21 | INFO | train_inner | epoch 001:    708 / 2763 bleu_loss=0.064, loss=0.064, ppl=1.05, wps=17330.2, ups=1.5, wpb=11518.3, bsz=356.2, num_updates=700, lr=2.93684e-05, gnorm=0.365, clip=88, loss_scale=0.5, train_wall=66, gb_free=15.6, wall=468
2021-11-20 17:10:29 | INFO | train_inner | epoch 001:    808 / 2763 bleu_loss=0.059, loss=0.059, ppl=1.04, wps=17313.2, ups=1.48, wpb=11698.9, bsz=348.1, num_updates=800, lr=2.90526e-05, gnorm=0.806, clip=88, loss_scale=0.5, train_wall=67, gb_free=24.6, wall=535
2021-11-20 17:11:33 | INFO | train_inner | epoch 001:    908 / 2763 bleu_loss=0.059, loss=0.059, ppl=1.04, wps=17804.3, ups=1.54, wpb=11536.2, bsz=370.2, num_updates=900, lr=2.87368e-05, gnorm=0.406, clip=88, loss_scale=0.5, train_wall=64, gb_free=16, wall=600
2021-11-20 17:12:40 | INFO | train_inner | epoch 001:   1008 / 2763 bleu_loss=0.057, loss=0.057, ppl=1.04, wps=17222.5, ups=1.51, wpb=11398.4, bsz=357.8, num_updates=1000, lr=2.84211e-05, gnorm=0.316, clip=82, loss_scale=0.5, train_wall=66, gb_free=16.4, wall=666
2021-11-20 17:13:42 | INFO | train_inner | epoch 001:   1108 / 2763 bleu_loss=0.063, loss=0.063, ppl=1.04, wps=17545.1, ups=1.61, wpb=10878.7, bsz=375.9, num_updates=1100, lr=2.81053e-05, gnorm=0.335, clip=79, loss_scale=0.5, train_wall=62, gb_free=16, wall=728
2021-11-20 17:14:46 | INFO | train_inner | epoch 001:   1208 / 2763 bleu_loss=0.213, loss=0.213, ppl=1.16, wps=17676.6, ups=1.56, wpb=11310.2, bsz=368.9, num_updates=1200, lr=2.77895e-05, gnorm=2.317, clip=88, loss_scale=0.5, train_wall=64, gb_free=15.3, wall=792
2021-11-20 17:15:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2021-11-20 17:15:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2021-11-20 17:15:52 | INFO | train_inner | epoch 001:   1310 / 2763 bleu_loss=0.828, loss=0.828, ppl=1.78, wps=17003.3, ups=1.51, wpb=11268.6, bsz=351.2, num_updates=1300, lr=2.74737e-05, gnorm=13.678, clip=99, loss_scale=0.125, train_wall=66, gb_free=21.4, wall=859
2021-11-20 17:16:57 | INFO | train_inner | epoch 001:   1410 / 2763 bleu_loss=0.871, loss=0.871, ppl=1.83, wps=18105.3, ups=1.54, wpb=11753.7, bsz=364.4, num_updates=1400, lr=2.71579e-05, gnorm=14.668, clip=100, loss_scale=0.125, train_wall=65, gb_free=26.7, wall=924
2021-11-20 17:18:02 | INFO | train_inner | epoch 001:   1510 / 2763 bleu_loss=1.12, loss=1.12, ppl=2.17, wps=17458.7, ups=1.52, wpb=11453, bsz=359.3, num_updates=1500, lr=2.68421e-05, gnorm=11.81, clip=99, loss_scale=0.125, train_wall=65, gb_free=20.8, wall=989
2021-11-20 17:18:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2021-11-20 17:18:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2021-11-20 17:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2021-11-20 17:19:12 | INFO | train_inner | epoch 001:   1613 / 2763 bleu_loss=1.634, loss=1.634, ppl=3.1, wps=16861.4, ups=1.43, wpb=11798.8, bsz=341.9, num_updates=1600, lr=2.65263e-05, gnorm=30.021, clip=100, loss_scale=0.0156, train_wall=70, gb_free=19.2, wall=1059
2021-11-20 17:20:18 | INFO | train_inner | epoch 001:   1713 / 2763 bleu_loss=1.174, loss=1.174, ppl=2.26, wps=17492.6, ups=1.52, wpb=11515.5, bsz=364.5, num_updates=1700, lr=2.62105e-05, gnorm=25.981, clip=99, loss_scale=0.0156, train_wall=65, gb_free=15.6, wall=1125
2021-11-20 17:21:26 | INFO | train_inner | epoch 001:   1813 / 2763 bleu_loss=0.773, loss=0.773, ppl=1.71, wps=17723.7, ups=1.48, wpb=11962.9, bsz=349.8, num_updates=1800, lr=2.58947e-05, gnorm=9.528, clip=100, loss_scale=0.0156, train_wall=67, gb_free=23.1, wall=1193
2021-11-20 17:22:31 | INFO | train_inner | epoch 001:   1913 / 2763 bleu_loss=0.696, loss=0.696, ppl=1.62, wps=17631.8, ups=1.54, wpb=11464.5, bsz=363.2, num_updates=1900, lr=2.55789e-05, gnorm=38.01, clip=99, loss_scale=0.0156, train_wall=65, gb_free=18.8, wall=1258
2021-11-20 17:23:35 | INFO | train_inner | epoch 001:   2013 / 2763 bleu_loss=0.878, loss=0.878, ppl=1.84, wps=17094.6, ups=1.56, wpb=10977, bsz=369.6, num_updates=2000, lr=2.52632e-05, gnorm=41.555, clip=100, loss_scale=0.0156, train_wall=64, gb_free=15.8, wall=1322
2021-11-20 17:24:40 | INFO | train_inner | epoch 001:   2113 / 2763 bleu_loss=0.325, loss=0.325, ppl=1.25, wps=18037.5, ups=1.55, wpb=11653.9, bsz=362.8, num_updates=2100, lr=2.49474e-05, gnorm=10.547, clip=100, loss_scale=0.0156, train_wall=64, gb_free=18.1, wall=1386
2021-11-20 17:25:45 | INFO | train_inner | epoch 001:   2213 / 2763 bleu_loss=0.008, loss=0.008, ppl=1.01, wps=17914.6, ups=1.52, wpb=11781.4, bsz=353.6, num_updates=2200, lr=2.46316e-05, gnorm=1.427, clip=46, loss_scale=0.0156, train_wall=65, gb_free=16.6, wall=1452
2021-11-20 17:26:50 | INFO | train_inner | epoch 001:   2313 / 2763 bleu_loss=0.003, loss=0.003, ppl=1, wps=17900.8, ups=1.56, wpb=11497.3, bsz=363.5, num_updates=2300, lr=2.43158e-05, gnorm=0.105, clip=18, loss_scale=0.0156, train_wall=64, gb_free=19.4, wall=1516
2021-11-20 17:27:55 | INFO | train_inner | epoch 001:   2413 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17840.1, ups=1.54, wpb=11591.6, bsz=353.5, num_updates=2400, lr=2.4e-05, gnorm=0.026, clip=4, loss_scale=0.0156, train_wall=65, gb_free=18.4, wall=1581
2021-11-20 17:28:57 | INFO | train_inner | epoch 001:   2513 / 2763 bleu_loss=0, loss=0, ppl=1, wps=18469.2, ups=1.61, wpb=11459.9, bsz=376.5, num_updates=2500, lr=2.36842e-05, gnorm=0.051, clip=7, loss_scale=0.0156, train_wall=62, gb_free=15.6, wall=1643
2021-11-20 17:30:00 | INFO | train_inner | epoch 001:   2613 / 2763 bleu_loss=0, loss=0, ppl=1, wps=17986.8, ups=1.57, wpb=11490.3, bsz=369.5, num_updates=2600, lr=2.33684e-05, gnorm=0.009, clip=0, loss_scale=0.0156, train_wall=64, gb_free=22.1, wall=1707
2021-11-20 17:31:04 | INFO | train_inner | epoch 001:   2713 / 2763 bleu_loss=0, loss=0, ppl=1, wps=18289.3, ups=1.57, wpb=11649.5, bsz=366.1, num_updates=2700, lr=2.30526e-05, gnorm=0.012, clip=2, loss_scale=0.0156, train_wall=63, gb_free=23.1, wall=1771
Train_lgy TIME: -1800.679
2021-11-20 17:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-11-20 17:32:55 | INFO | valid | epoch 001 | valid on 'valid' subset | bleu_loss 0 | loss 0 | ppl 1 | bleu 0 | wps 1080.2 | wpb 2579.4 | bsz 91 | num_updates 2750
valid_lgy time: 79.089
2021-11-20 17:32:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2750 updates
2021-11-20 17:33:00 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt
2021-11-20 17:33:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt (epoch 1 @ 2750 updates, score 0.0) (writing took 10.973832089453936 seconds)
2021-11-20 17:33:06 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-11-20 17:33:06 | INFO | train | epoch 001 | bleu_loss 0.45 | loss 0.45 | ppl 1.37 | wps 16793.1 | ups 1.46 | wpb 11510.3 | bsz 361.9 | num_updates 2750 | lr 2.28947e-05 | gnorm 7.831 | clip 75.6 | loss_scale 0.0156 | train_wall 1790 | gb_free 23.2 | wall 1892
2021-11-20 17:33:06 | INFO | fairseq_cli.train | done training in 1890.8 seconds
lgy-ngram-2
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 17:33:10 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='2', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='2', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 17:33:10 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 17:33:10 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 17:33:10 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 17:33:10 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 17:33:10 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 17:33:15 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 17:33:15 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 17:33:15 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 17:33:15 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 17:33:15 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 17:33:18 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 17:33:18 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 17:33:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 17:33:18 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 17:33:18 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 17:33:18 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 17:33:18 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 17:33:18 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 17:33:19 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 17:33:19 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 17:33:19 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 17:33:19 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 17:33:19 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 17:33:20 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 17:33:20 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 17:33:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 17:33:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 17:33:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 17:33:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 17:33:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 17:33:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 17:33:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2021-11-20 17:34:33.078731: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-11-20 17:34:33.078836: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-11-20 17:34:33.078851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-11-20 17:34:33 | INFO | train_inner | epoch 001:    107 / 2763 bleu_loss=3.532, loss=3.532, ppl=11.57, wps=17074.5, ups=1.51, wpb=11294.1, bsz=363.2, num_updates=100, lr=6e-06, gnorm=3.872, clip=100, loss_scale=1, train_wall=72, gb_free=23.2, wall=74
2021-11-20 17:35:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2021-11-20 17:35:39 | INFO | train_inner | epoch 001:    208 / 2763 bleu_loss=1.372, loss=1.372, ppl=2.59, wps=17361.4, ups=1.53, wpb=11371.9, bsz=372.3, num_updates=200, lr=1.2e-05, gnorm=3.679, clip=100, loss_scale=0.5, train_wall=65, gb_free=23.1, wall=141
2021-11-20 17:36:46 | INFO | train_inner | epoch 001:    308 / 2763 bleu_loss=0.251, loss=0.251, ppl=1.19, wps=17624.4, ups=1.48, wpb=11911.8, bsz=351.1, num_updates=300, lr=1.8e-05, gnorm=1.036, clip=100, loss_scale=0.5, train_wall=67, gb_free=20.8, wall=208
2021-11-20 17:37:52 | INFO | train_inner | epoch 001:    408 / 2763 bleu_loss=0.182, loss=0.182, ppl=1.13, wps=17737.4, ups=1.52, wpb=11686.1, bsz=351.2, num_updates=400, lr=2.4e-05, gnorm=0.398, clip=100, loss_scale=0.5, train_wall=66, gb_free=21.6, wall=274
2021-11-20 17:38:55 | INFO | train_inner | epoch 001:    508 / 2763 bleu_loss=0.189, loss=0.189, ppl=1.14, wps=18024.3, ups=1.6, wpb=11278.9, bsz=376, num_updates=500, lr=3e-05, gnorm=1.129, clip=95, loss_scale=0.5, train_wall=62, gb_free=15.1, wall=337
2021-11-20 17:39:59 | INFO | train_inner | epoch 001:    608 / 2763 bleu_loss=0.167, loss=0.167, ppl=1.12, wps=18009.3, ups=1.55, wpb=11607.6, bsz=365.5, num_updates=600, lr=2.96842e-05, gnorm=0.28, clip=93, loss_scale=0.5, train_wall=64, gb_free=13.5, wall=401
2021-11-20 17:41:06 | INFO | train_inner | epoch 001:    708 / 2763 bleu_loss=0.164, loss=0.164, ppl=1.12, wps=17327.7, ups=1.5, wpb=11518.3, bsz=356.2, num_updates=700, lr=2.93684e-05, gnorm=0.259, clip=87, loss_scale=0.5, train_wall=66, gb_free=15.6, wall=468
2021-11-20 17:42:14 | INFO | train_inner | epoch 001:    808 / 2763 bleu_loss=0.159, loss=0.159, ppl=1.12, wps=17250.8, ups=1.47, wpb=11698.9, bsz=348.1, num_updates=800, lr=2.90526e-05, gnorm=1.034, clip=84, loss_scale=0.5, train_wall=67, gb_free=24.6, wall=536
2021-11-20 17:43:19 | INFO | train_inner | epoch 001:    908 / 2763 bleu_loss=0.171, loss=0.171, ppl=1.13, wps=17669.4, ups=1.53, wpb=11536.2, bsz=370.2, num_updates=900, lr=2.87368e-05, gnorm=0.207, clip=79, loss_scale=0.5, train_wall=65, gb_free=16, wall=601
2021-11-20 17:44:26 | INFO | train_inner | epoch 001:   1008 / 2763 bleu_loss=0.163, loss=0.163, ppl=1.12, wps=17130.5, ups=1.5, wpb=11398.4, bsz=357.8, num_updates=1000, lr=2.84211e-05, gnorm=0.42, clip=75, loss_scale=0.5, train_wall=66, gb_free=16.4, wall=667
2021-11-20 17:45:28 | INFO | train_inner | epoch 001:   1108 / 2763 bleu_loss=0.184, loss=0.184, ppl=1.14, wps=17551.4, ups=1.61, wpb=10878.7, bsz=375.9, num_updates=1100, lr=2.81053e-05, gnorm=0.509, clip=78, loss_scale=0.5, train_wall=62, gb_free=16, wall=729
2021-11-20 17:46:32 | INFO | train_inner | epoch 001:   1208 / 2763 bleu_loss=0.171, loss=0.171, ppl=1.13, wps=17668, ups=1.56, wpb=11310.2, bsz=368.9, num_updates=1200, lr=2.77895e-05, gnorm=0.266, clip=78, loss_scale=0.5, train_wall=64, gb_free=15.3, wall=793
2021-11-20 17:47:36 | INFO | train_inner | epoch 001:   1308 / 2763 bleu_loss=0.165, loss=0.165, ppl=1.12, wps=17240.3, ups=1.54, wpb=11186.7, bsz=351.9, num_updates=1300, lr=2.74737e-05, gnorm=0.218, clip=73, loss_scale=0.5, train_wall=65, gb_free=16.5, wall=858
2021-11-20 17:48:41 | INFO | train_inner | epoch 001:   1408 / 2763 bleu_loss=0.151, loss=0.151, ppl=1.11, wps=18290.6, ups=1.54, wpb=11853.8, bsz=362.5, num_updates=1400, lr=2.71579e-05, gnorm=0.184, clip=71, loss_scale=0.5, train_wall=64, gb_free=20.8, wall=923
2021-11-20 17:49:47 | INFO | train_inner | epoch 001:   1508 / 2763 bleu_loss=0.16, loss=0.16, ppl=1.12, wps=17500.9, ups=1.52, wpb=11483.5, bsz=360.4, num_updates=1500, lr=2.68421e-05, gnorm=0.234, clip=79, loss_scale=0.5, train_wall=65, gb_free=21.6, wall=989
2021-11-20 17:50:55 | INFO | train_inner | epoch 001:   1608 / 2763 bleu_loss=0.148, loss=0.148, ppl=1.11, wps=17415.3, ups=1.48, wpb=11790.9, bsz=344.1, num_updates=1600, lr=2.65263e-05, gnorm=0.17, clip=68, loss_scale=0.5, train_wall=67, gb_free=25.9, wall=1056
2021-11-20 17:52:01 | INFO | train_inner | epoch 001:   1708 / 2763 bleu_loss=0.157, loss=0.157, ppl=1.12, wps=17460.1, ups=1.51, wpb=11542.9, bsz=360.4, num_updates=1700, lr=2.62105e-05, gnorm=0.18, clip=74, loss_scale=0.5, train_wall=66, gb_free=19.3, wall=1123
2021-11-20 17:52:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2021-11-20 17:53:09 | INFO | train_inner | epoch 001:   1809 / 2763 bleu_loss=0.154, loss=0.154, ppl=1.11, wps=17435, ups=1.47, wpb=11835.4, bsz=352.2, num_updates=1800, lr=2.58947e-05, gnorm=0.19, clip=74, loss_scale=0.25, train_wall=68, gb_free=15.9, wall=1190
2021-11-20 17:54:14 | INFO | train_inner | epoch 001:   1909 / 2763 bleu_loss=0.161, loss=0.161, ppl=1.12, wps=17823.3, ups=1.53, wpb=11621.2, bsz=361.3, num_updates=1900, lr=2.55789e-05, gnorm=0.18, clip=65, loss_scale=0.25, train_wall=65, gb_free=18.5, wall=1256
2021-11-20 17:55:17 | INFO | train_inner | epoch 001:   2009 / 2763 bleu_loss=0.168, loss=0.168, ppl=1.12, wps=17230.4, ups=1.59, wpb=10853.9, bsz=373.2, num_updates=2000, lr=2.52632e-05, gnorm=0.21, clip=75, loss_scale=0.25, train_wall=63, gb_free=20.4, wall=1319
2021-11-20 17:56:22 | INFO | train_inner | epoch 001:   2109 / 2763 bleu_loss=0.15, loss=0.15, ppl=1.11, wps=17906.5, ups=1.53, wpb=11717.6, bsz=359.9, num_updates=2100, lr=2.49474e-05, gnorm=0.153, clip=63, loss_scale=0.25, train_wall=65, gb_free=20, wall=1384
2021-11-20 17:57:28 | INFO | train_inner | epoch 001:   2209 / 2763 bleu_loss=0.153, loss=0.153, ppl=1.11, wps=17666.6, ups=1.52, wpb=11616.8, bsz=355.3, num_updates=2200, lr=2.46316e-05, gnorm=0.178, clip=65, loss_scale=0.25, train_wall=65, gb_free=15.7, wall=1450
2021-11-20 17:58:33 | INFO | train_inner | epoch 001:   2309 / 2763 bleu_loss=0.148, loss=0.148, ppl=1.11, wps=17750.8, ups=1.53, wpb=11630.8, bsz=361.8, num_updates=2300, lr=2.43158e-05, gnorm=0.171, clip=64, loss_scale=0.25, train_wall=65, gb_free=22.1, wall=1515
2021-11-20 17:59:40 | INFO | train_inner | epoch 001:   2409 / 2763 bleu_loss=0.149, loss=0.149, ppl=1.11, wps=17558.3, ups=1.51, wpb=11606.5, bsz=352.9, num_updates=2400, lr=2.4e-05, gnorm=0.163, clip=66, loss_scale=0.25, train_wall=66, gb_free=15.9, wall=1581
2021-11-20 18:00:42 | INFO | train_inner | epoch 001:   2509 / 2763 bleu_loss=0.158, loss=0.158, ppl=1.12, wps=18415.6, ups=1.61, wpb=11427.2, bsz=380.2, num_updates=2500, lr=2.36842e-05, gnorm=0.369, clip=71, loss_scale=0.25, train_wall=62, gb_free=15.9, wall=1644
2021-11-20 18:01:46 | INFO | train_inner | epoch 001:   2609 / 2763 bleu_loss=0.153, loss=0.153, ppl=1.11, wps=17915, ups=1.54, wpb=11600.9, bsz=364.5, num_updates=2600, lr=2.33684e-05, gnorm=0.203, clip=67, loss_scale=0.25, train_wall=64, gb_free=18.9, wall=1708
2021-11-20 18:02:50 | INFO | train_inner | epoch 001:   2709 / 2763 bleu_loss=0.158, loss=0.158, ppl=1.12, wps=18072.9, ups=1.56, wpb=11548.6, bsz=367.4, num_updates=2700, lr=2.30526e-05, gnorm=0.212, clip=65, loss_scale=0.25, train_wall=64, gb_free=17, wall=1772
Train_lgy TIME: -1805.009
2021-11-20 18:03:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-11-20 18:04:45 | INFO | valid | epoch 001 | valid on 'valid' subset | bleu_loss 0.02 | loss 0.02 | ppl 1.01 | bleu 0 | wps 1060.7 | wpb 2579.4 | bsz 91 | num_updates 2754
valid_lgy time: 80.503
2021-11-20 18:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2754 updates
2021-11-20 18:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt
2021-11-20 18:04:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt (epoch 1 @ 2754 updates, score 0.0) (writing took 11.458894461393356 seconds)
2021-11-20 18:04:57 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-11-20 18:04:57 | INFO | train | epoch 001 | bleu_loss 0.328 | loss 0.328 | ppl 1.26 | wps 16766.2 | ups 1.46 | wpb 11512.7 | bsz 361.9 | num_updates 2754 | lr 2.28821e-05 | gnorm 0.587 | clip 77.7 | loss_scale 0.25 | train_wall 1794 | gb_free 23.2 | wall 1899
2021-11-20 18:04:57 | INFO | fairseq_cli.train | done training in 1897.0 seconds
lgy-ngram-2
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 18:05:01 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='2', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='2', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 18:05:01 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 18:05:01 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 18:05:01 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 18:05:01 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 18:05:01 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 18:05:06 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 18:05:06 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 18:05:06 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 18:05:06 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 18:05:06 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 18:05:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 18:05:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 18:05:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 18:05:09 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 18:05:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 18:05:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 18:05:09 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 18:05:09 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 18:05:11 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 18:05:11 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 18:05:11 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 18:05:11 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 18:05:11 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 18:05:11 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 18:05:11 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 18:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 18:05:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 18:05:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 18:05:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 18:05:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 18:05:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 18:05:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2021-11-20 18:06:24.474347: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-11-20 18:06:24.474446: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-11-20 18:06:24.474459: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-11-20 18:06:25 | INFO | train_inner | epoch 001:    107 / 2763 bleu_loss=3.532, loss=3.532, ppl=11.57, wps=17057.9, ups=1.51, wpb=11294.1, bsz=363.2, num_updates=100, lr=6e-06, gnorm=3.898, clip=100, loss_scale=1, train_wall=72, gb_free=23.2, wall=74
2021-11-20 18:06:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2021-11-20 18:07:30 | INFO | train_inner | epoch 001:    208 / 2763 bleu_loss=1.38, loss=1.38, ppl=2.6, wps=17361.4, ups=1.54, wpb=11284.9, bsz=374, num_updates=200, lr=1.2e-05, gnorm=3.982, clip=100, loss_scale=0.5, train_wall=65, gb_free=23.1, wall=140
2021-11-20 18:08:37 | INFO | train_inner | epoch 001:    308 / 2763 bleu_loss=0.267, loss=0.267, ppl=1.2, wps=17591.8, ups=1.48, wpb=11911.8, bsz=351.1, num_updates=300, lr=1.8e-05, gnorm=1.147, clip=100, loss_scale=0.5, train_wall=67, gb_free=20.8, wall=208
2021-11-20 18:09:44 | INFO | train_inner | epoch 001:    408 / 2763 bleu_loss=0.181, loss=0.181, ppl=1.13, wps=17692.2, ups=1.51, wpb=11686.1, bsz=351.2, num_updates=400, lr=2.4e-05, gnorm=0.396, clip=99, loss_scale=0.5, train_wall=66, gb_free=21.6, wall=274
2021-11-20 18:10:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2021-11-20 18:10:47 | INFO | train_inner | epoch 001:    509 / 2763 bleu_loss=0.189, loss=0.189, ppl=1.14, wps=17739.4, ups=1.57, wpb=11299.2, bsz=376, num_updates=500, lr=3e-05, gnorm=0.948, clip=95, loss_scale=0.25, train_wall=63, gb_free=17.4, wall=338
2021-11-20 18:11:52 | INFO | train_inner | epoch 001:    609 / 2763 bleu_loss=0.19, loss=0.19, ppl=1.14, wps=17985.5, ups=1.55, wpb=11574.2, bsz=367.8, num_updates=600, lr=2.96842e-05, gnorm=0.739, clip=95, loss_scale=0.25, train_wall=64, gb_free=25.7, wall=402
2021-11-20 18:12:58 | INFO | train_inner | epoch 001:    709 / 2763 bleu_loss=0.164, loss=0.164, ppl=1.12, wps=17364.2, ups=1.51, wpb=11522, bsz=356.2, num_updates=700, lr=2.93684e-05, gnorm=0.237, clip=82, loss_scale=0.25, train_wall=66, gb_free=22.4, wall=469
2021-11-20 18:14:06 | INFO | train_inner | epoch 001:    809 / 2763 bleu_loss=0.158, loss=0.158, ppl=1.12, wps=17316.4, ups=1.48, wpb=11723.4, bsz=346.3, num_updates=800, lr=2.90526e-05, gnorm=0.25, clip=81, loss_scale=0.25, train_wall=67, gb_free=19.3, wall=536
2021-11-20 18:15:11 | INFO | train_inner | epoch 001:    909 / 2763 bleu_loss=0.171, loss=0.171, ppl=1.13, wps=17793.6, ups=1.54, wpb=11567, bsz=370.5, num_updates=900, lr=2.87368e-05, gnorm=0.254, clip=82, loss_scale=0.25, train_wall=65, gb_free=14.5, wall=601
2021-11-20 18:16:17 | INFO | train_inner | epoch 001:   1009 / 2763 bleu_loss=0.163, loss=0.163, ppl=1.12, wps=17124.8, ups=1.51, wpb=11356.5, bsz=358.7, num_updates=1000, lr=2.84211e-05, gnorm=0.202, clip=78, loss_scale=0.25, train_wall=66, gb_free=22.3, wall=668
2021-11-20 18:17:20 | INFO | train_inner | epoch 001:   1109 / 2763 bleu_loss=0.18, loss=0.18, ppl=1.13, wps=17435.2, ups=1.6, wpb=10920.4, bsz=375.2, num_updates=1100, lr=2.81053e-05, gnorm=0.226, clip=80, loss_scale=0.25, train_wall=62, gb_free=19.5, wall=730
2021-11-20 18:18:23 | INFO | train_inner | epoch 001:   1209 / 2763 bleu_loss=0.174, loss=0.174, ppl=1.13, wps=17716.6, ups=1.57, wpb=11305.2, bsz=367.9, num_updates=1200, lr=2.77895e-05, gnorm=0.305, clip=81, loss_scale=0.25, train_wall=63, gb_free=16.4, wall=794
2021-11-20 18:19:29 | INFO | train_inner | epoch 001:   1309 / 2763 bleu_loss=0.164, loss=0.164, ppl=1.12, wps=17224.2, ups=1.54, wpb=11212.8, bsz=352.8, num_updates=1300, lr=2.74737e-05, gnorm=0.208, clip=70, loss_scale=0.25, train_wall=65, gb_free=19.2, wall=859
2021-11-20 18:20:33 | INFO | train_inner | epoch 001:   1409 / 2763 bleu_loss=0.152, loss=0.152, ppl=1.11, wps=18310.2, ups=1.55, wpb=11807, bsz=362.9, num_updates=1400, lr=2.71579e-05, gnorm=0.196, clip=72, loss_scale=0.25, train_wall=64, gb_free=19.7, wall=924
2021-11-20 18:21:39 | INFO | train_inner | epoch 001:   1509 / 2763 bleu_loss=0.162, loss=0.162, ppl=1.12, wps=17452.7, ups=1.53, wpb=11441, bsz=360.4, num_updates=1500, lr=2.68421e-05, gnorm=0.216, clip=78, loss_scale=0.25, train_wall=65, gb_free=19, wall=989
2021-11-20 18:22:46 | INFO | train_inner | epoch 001:   1609 / 2763 bleu_loss=0.146, loss=0.146, ppl=1.11, wps=17536.6, ups=1.48, wpb=11861.2, bsz=342.2, num_updates=1600, lr=2.65263e-05, gnorm=0.154, clip=66, loss_scale=0.25, train_wall=67, gb_free=16.1, wall=1057
2021-11-20 18:23:53 | INFO | train_inner | epoch 001:   1709 / 2763 bleu_loss=0.161, loss=0.161, ppl=1.12, wps=17320.2, ups=1.51, wpb=11496.5, bsz=361, num_updates=1700, lr=2.62105e-05, gnorm=0.414, clip=79, loss_scale=0.25, train_wall=66, gb_free=15.8, wall=1123
2021-11-20 18:25:00 | INFO | train_inner | epoch 001:   1809 / 2763 bleu_loss=0.154, loss=0.154, ppl=1.11, wps=17654.7, ups=1.49, wpb=11868.5, bsz=353.3, num_updates=1800, lr=2.58947e-05, gnorm=0.211, clip=73, loss_scale=0.25, train_wall=67, gb_free=15.9, wall=1191
2021-11-20 18:26:05 | INFO | train_inner | epoch 001:   1909 / 2763 bleu_loss=0.161, loss=0.161, ppl=1.12, wps=17820.7, ups=1.53, wpb=11621.2, bsz=361.3, num_updates=1900, lr=2.55789e-05, gnorm=0.177, clip=63, loss_scale=0.25, train_wall=65, gb_free=18.5, wall=1256
2021-11-20 18:27:09 | INFO | train_inner | epoch 001:   2009 / 2763 bleu_loss=0.168, loss=0.168, ppl=1.12, wps=17017.8, ups=1.57, wpb=10853.9, bsz=373.2, num_updates=2000, lr=2.52632e-05, gnorm=0.222, clip=72, loss_scale=0.25, train_wall=63, gb_free=20.4, wall=1320
2021-11-20 18:28:14 | INFO | train_inner | epoch 001:   2109 / 2763 bleu_loss=0.151, loss=0.151, ppl=1.11, wps=17952.8, ups=1.53, wpb=11717.6, bsz=359.9, num_updates=2100, lr=2.49474e-05, gnorm=0.153, clip=64, loss_scale=0.25, train_wall=65, gb_free=20, wall=1385
2021-11-20 18:29:19 | INFO | train_inner | epoch 001:   2209 / 2763 bleu_loss=0.153, loss=0.153, ppl=1.11, wps=17773.8, ups=1.53, wpb=11616.8, bsz=355.3, num_updates=2200, lr=2.46316e-05, gnorm=0.185, clip=64, loss_scale=0.25, train_wall=65, gb_free=15.7, wall=1450
2021-11-20 18:30:25 | INFO | train_inner | epoch 001:   2309 / 2763 bleu_loss=0.149, loss=0.149, ppl=1.11, wps=17663.2, ups=1.52, wpb=11630.8, bsz=361.8, num_updates=2300, lr=2.43158e-05, gnorm=0.205, clip=64, loss_scale=0.25, train_wall=66, gb_free=22.1, wall=1516
2021-11-20 18:31:31 | INFO | train_inner | epoch 001:   2409 / 2763 bleu_loss=0.149, loss=0.149, ppl=1.11, wps=17587.9, ups=1.52, wpb=11606.5, bsz=352.9, num_updates=2400, lr=2.4e-05, gnorm=0.167, clip=66, loss_scale=0.25, train_wall=66, gb_free=15.9, wall=1582
2021-11-20 18:32:34 | INFO | train_inner | epoch 001:   2509 / 2763 bleu_loss=0.158, loss=0.158, ppl=1.12, wps=18265.1, ups=1.6, wpb=11427.2, bsz=380.2, num_updates=2500, lr=2.36842e-05, gnorm=0.198, clip=69, loss_scale=0.25, train_wall=62, gb_free=15.9, wall=1645
2021-11-20 18:33:39 | INFO | train_inner | epoch 001:   2609 / 2763 bleu_loss=0.153, loss=0.153, ppl=1.11, wps=17831.7, ups=1.54, wpb=11600.9, bsz=364.5, num_updates=2600, lr=2.33684e-05, gnorm=0.194, clip=66, loss_scale=0.25, train_wall=65, gb_free=18.9, wall=1710
2021-11-20 18:34:43 | INFO | train_inner | epoch 001:   2709 / 2763 bleu_loss=0.158, loss=0.158, ppl=1.12, wps=18066.9, ups=1.56, wpb=11548.6, bsz=367.4, num_updates=2700, lr=2.30526e-05, gnorm=0.145, clip=60, loss_scale=0.25, train_wall=64, gb_free=17, wall=1774
Train_lgy TIME: -1806.326
2021-11-20 18:35:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-11-20 18:36:38 | INFO | valid | epoch 001 | valid on 'valid' subset | bleu_loss 0.019 | loss 0.019 | ppl 1.01 | bleu 0 | wps 1057.4 | wpb 2579.4 | bsz 91 | num_updates 2754
valid_lgy time: 80.776
2021-11-20 18:36:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2754 updates
2021-11-20 18:36:44 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt
2021-11-20 18:36:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt (epoch 1 @ 2754 updates, score 0.0) (writing took 11.419881153851748 seconds)
2021-11-20 18:36:50 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-11-20 18:36:50 | INFO | train | epoch 001 | bleu_loss 0.33 | loss 0.33 | ppl 1.26 | wps 16750.6 | ups 1.46 | wpb 11510.8 | bsz 362 | num_updates 2754 | lr 2.28821e-05 | gnorm 0.57 | clip 77.5 | loss_scale 0.25 | train_wall 1795 | gb_free 23.2 | wall 1900
2021-11-20 18:36:50 | INFO | fairseq_cli.train | done training in 1898.6 seconds
lgy-ngram-2
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 18:36:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='2', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='2', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 18:36:54 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 18:36:54 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 18:36:54 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 18:36:54 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 18:36:54 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 18:36:59 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 18:36:59 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 18:36:59 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 18:36:59 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 18:36:59 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 18:37:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 18:37:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 18:37:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 18:37:03 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 18:37:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 18:37:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 18:37:03 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 18:37:03 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 18:37:04 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 18:37:04 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 18:37:04 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 18:37:04 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 18:37:04 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 18:37:04 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 18:37:04 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 18:37:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 18:37:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 18:37:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 18:37:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 18:37:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 18:37:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 18:37:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2021-11-20 18:38:17.290672: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-11-20 18:38:17.290776: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-11-20 18:38:17.290790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-11-20 18:38:18 | INFO | train_inner | epoch 001:    107 / 2763 bleu_loss=3.531, loss=3.531, ppl=11.56, wps=17114.5, ups=1.51, wpb=11294.1, bsz=363.2, num_updates=100, lr=6e-06, gnorm=3.854, clip=100, loss_scale=1, train_wall=71, gb_free=23.2, wall=73
2021-11-20 18:38:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2021-11-20 18:39:23 | INFO | train_inner | epoch 001:    208 / 2763 bleu_loss=1.365, loss=1.365, ppl=2.58, wps=17368.4, ups=1.53, wpb=11371.9, bsz=372.3, num_updates=200, lr=1.2e-05, gnorm=3.681, clip=100, loss_scale=0.5, train_wall=65, gb_free=23.1, wall=141
2021-11-20 18:40:31 | INFO | train_inner | epoch 001:    308 / 2763 bleu_loss=0.266, loss=0.266, ppl=1.2, wps=17636.9, ups=1.48, wpb=11911.8, bsz=351.1, num_updates=300, lr=1.8e-05, gnorm=1.287, clip=100, loss_scale=0.5, train_wall=67, gb_free=20.8, wall=208
2021-11-20 18:41:36 | INFO | train_inner | epoch 001:    408 / 2763 bleu_loss=0.183, loss=0.183, ppl=1.14, wps=17773.2, ups=1.52, wpb=11686.1, bsz=351.2, num_updates=400, lr=2.4e-05, gnorm=0.369, clip=100, loss_scale=0.5, train_wall=65, gb_free=21.6, wall=274
2021-11-20 18:42:39 | INFO | train_inner | epoch 001:    508 / 2763 bleu_loss=0.186, loss=0.186, ppl=1.14, wps=17995.3, ups=1.6, wpb=11278.9, bsz=376, num_updates=500, lr=3e-05, gnorm=0.296, clip=93, loss_scale=0.5, train_wall=62, gb_free=15.1, wall=337
2021-11-20 18:43:44 | INFO | train_inner | epoch 001:    608 / 2763 bleu_loss=0.166, loss=0.166, ppl=1.12, wps=17990.9, ups=1.55, wpb=11607.6, bsz=365.5, num_updates=600, lr=2.96842e-05, gnorm=0.261, clip=91, loss_scale=0.5, train_wall=64, gb_free=13.5, wall=401
2021-11-20 18:44:49 | INFO | train_inner | epoch 001:    708 / 2763 bleu_loss=0.164, loss=0.164, ppl=1.12, wps=17554.3, ups=1.52, wpb=11518.3, bsz=356.2, num_updates=700, lr=2.93684e-05, gnorm=0.221, clip=83, loss_scale=0.5, train_wall=65, gb_free=15.6, wall=467
2021-11-20 18:45:56 | INFO | train_inner | epoch 001:    808 / 2763 bleu_loss=0.159, loss=0.159, ppl=1.12, wps=17470.5, ups=1.49, wpb=11698.9, bsz=348.1, num_updates=800, lr=2.90526e-05, gnorm=0.268, clip=83, loss_scale=0.5, train_wall=67, gb_free=24.6, wall=534
2021-11-20 18:46:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2021-11-20 18:47:02 | INFO | train_inner | epoch 001:    909 / 2763 bleu_loss=0.181, loss=0.181, ppl=1.13, wps=17585.4, ups=1.52, wpb=11540.3, bsz=370.2, num_updates=900, lr=2.87368e-05, gnorm=0.608, clip=86, loss_scale=0.25, train_wall=65, gb_free=14.5, wall=599
2021-11-20 18:48:08 | INFO | train_inner | epoch 001:   1009 / 2763 bleu_loss=0.163, loss=0.163, ppl=1.12, wps=17239.7, ups=1.52, wpb=11356.5, bsz=358.7, num_updates=1000, lr=2.84211e-05, gnorm=0.21, clip=81, loss_scale=0.25, train_wall=66, gb_free=22.3, wall=665
2021-11-20 18:49:10 | INFO | train_inner | epoch 001:   1109 / 2763 bleu_loss=0.182, loss=0.182, ppl=1.13, wps=17577.2, ups=1.61, wpb=10920.4, bsz=375.2, num_updates=1100, lr=2.81053e-05, gnorm=0.315, clip=80, loss_scale=0.25, train_wall=62, gb_free=19.5, wall=727
2021-11-20 18:50:14 | INFO | train_inner | epoch 001:   1209 / 2763 bleu_loss=0.173, loss=0.173, ppl=1.13, wps=17686.3, ups=1.56, wpb=11305.2, bsz=367.9, num_updates=1200, lr=2.77895e-05, gnorm=0.291, clip=77, loss_scale=0.25, train_wall=64, gb_free=16.4, wall=791
2021-11-20 18:51:18 | INFO | train_inner | epoch 001:   1309 / 2763 bleu_loss=0.165, loss=0.165, ppl=1.12, wps=17397.5, ups=1.55, wpb=11212.8, bsz=352.8, num_updates=1300, lr=2.74737e-05, gnorm=0.266, clip=78, loss_scale=0.25, train_wall=64, gb_free=19.2, wall=856
2021-11-20 18:52:23 | INFO | train_inner | epoch 001:   1409 / 2763 bleu_loss=0.152, loss=0.152, ppl=1.11, wps=18247.2, ups=1.55, wpb=11807, bsz=362.9, num_updates=1400, lr=2.71579e-05, gnorm=0.19, clip=69, loss_scale=0.25, train_wall=64, gb_free=19.7, wall=920
2021-11-20 18:53:28 | INFO | train_inner | epoch 001:   1509 / 2763 bleu_loss=0.161, loss=0.161, ppl=1.12, wps=17554.2, ups=1.53, wpb=11441, bsz=360.4, num_updates=1500, lr=2.68421e-05, gnorm=0.192, clip=74, loss_scale=0.25, train_wall=65, gb_free=19, wall=986
2021-11-20 18:54:36 | INFO | train_inner | epoch 001:   1609 / 2763 bleu_loss=0.146, loss=0.146, ppl=1.11, wps=17447.9, ups=1.47, wpb=11861.2, bsz=342.2, num_updates=1600, lr=2.65263e-05, gnorm=0.148, clip=63, loss_scale=0.25, train_wall=68, gb_free=16.1, wall=1054
2021-11-20 18:55:42 | INFO | train_inner | epoch 001:   1709 / 2763 bleu_loss=0.157, loss=0.157, ppl=1.12, wps=17527.1, ups=1.52, wpb=11496.5, bsz=361, num_updates=1700, lr=2.62105e-05, gnorm=0.208, clip=76, loss_scale=0.25, train_wall=65, gb_free=15.8, wall=1119
2021-11-20 18:56:49 | INFO | train_inner | epoch 001:   1809 / 2763 bleu_loss=0.154, loss=0.154, ppl=1.11, wps=17732.1, ups=1.49, wpb=11868.5, bsz=353.3, num_updates=1800, lr=2.58947e-05, gnorm=0.205, clip=74, loss_scale=0.25, train_wall=67, gb_free=15.9, wall=1186
2021-11-20 18:57:53 | INFO | train_inner | epoch 001:   1909 / 2763 bleu_loss=0.16, loss=0.16, ppl=1.12, wps=17992.5, ups=1.55, wpb=11621.2, bsz=361.3, num_updates=1900, lr=2.55789e-05, gnorm=0.172, clip=62, loss_scale=0.25, train_wall=64, gb_free=18.5, wall=1251
2021-11-20 18:58:56 | INFO | train_inner | epoch 001:   2009 / 2763 bleu_loss=0.174, loss=0.174, ppl=1.13, wps=17216, ups=1.59, wpb=10853.9, bsz=373.2, num_updates=2000, lr=2.52632e-05, gnorm=0.245, clip=74, loss_scale=0.25, train_wall=63, gb_free=20.4, wall=1314
2021-11-20 19:00:01 | INFO | train_inner | epoch 001:   2109 / 2763 bleu_loss=0.151, loss=0.151, ppl=1.11, wps=17993.9, ups=1.54, wpb=11717.6, bsz=359.9, num_updates=2100, lr=2.49474e-05, gnorm=0.157, clip=63, loss_scale=0.25, train_wall=65, gb_free=20, wall=1379
2021-11-20 19:01:07 | INFO | train_inner | epoch 001:   2209 / 2763 bleu_loss=0.153, loss=0.153, ppl=1.11, wps=17815.5, ups=1.53, wpb=11616.8, bsz=355.3, num_updates=2200, lr=2.46316e-05, gnorm=0.222, clip=63, loss_scale=0.25, train_wall=65, gb_free=15.7, wall=1444
2021-11-20 19:02:12 | INFO | train_inner | epoch 001:   2309 / 2763 bleu_loss=0.149, loss=0.149, ppl=1.11, wps=17830.5, ups=1.53, wpb=11630.8, bsz=361.8, num_updates=2300, lr=2.43158e-05, gnorm=0.217, clip=65, loss_scale=0.25, train_wall=65, gb_free=22.1, wall=1509
2021-11-20 19:03:18 | INFO | train_inner | epoch 001:   2409 / 2763 bleu_loss=0.149, loss=0.149, ppl=1.11, wps=17645.7, ups=1.52, wpb=11606.5, bsz=352.9, num_updates=2400, lr=2.4e-05, gnorm=0.164, clip=64, loss_scale=0.25, train_wall=65, gb_free=15.9, wall=1575
2021-11-20 19:04:19 | INFO | train_inner | epoch 001:   2509 / 2763 bleu_loss=0.158, loss=0.158, ppl=1.12, wps=18535.6, ups=1.62, wpb=11427.2, bsz=380.2, num_updates=2500, lr=2.36842e-05, gnorm=0.201, clip=73, loss_scale=0.25, train_wall=61, gb_free=15.9, wall=1637
2021-11-20 19:05:24 | INFO | train_inner | epoch 001:   2609 / 2763 bleu_loss=0.153, loss=0.153, ppl=1.11, wps=17892.1, ups=1.54, wpb=11600.9, bsz=364.5, num_updates=2600, lr=2.33684e-05, gnorm=0.207, clip=63, loss_scale=0.25, train_wall=65, gb_free=18.9, wall=1702
2021-11-20 19:06:27 | INFO | train_inner | epoch 001:   2709 / 2763 bleu_loss=0.158, loss=0.158, ppl=1.12, wps=18373.4, ups=1.59, wpb=11548.6, bsz=367.4, num_updates=2700, lr=2.30526e-05, gnorm=0.183, clip=64, loss_scale=0.25, train_wall=63, gb_free=17, wall=1764
Train_lgy TIME: -1796.952
2021-11-20 19:07:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-11-20 19:08:21 | INFO | valid | epoch 001 | valid on 'valid' subset | bleu_loss 0.02 | loss 0.02 | ppl 1.01 | bleu 0 | wps 1065.7 | wpb 2579.4 | bsz 91 | num_updates 2754
valid_lgy time: 80.161
2021-11-20 19:08:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2754 updates
2021-11-20 19:08:28 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt
2021-11-20 19:08:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt (epoch 1 @ 2754 updates, score 0.0) (writing took 11.501709677278996 seconds)
2021-11-20 19:08:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-11-20 19:08:33 | INFO | train | epoch 001 | bleu_loss 0.329 | loss 0.329 | ppl 1.26 | wps 16838.3 | ups 1.46 | wpb 11512.5 | bsz 361.9 | num_updates 2754 | lr 2.28821e-05 | gnorm 0.534 | clip 77.5 | loss_scale 0.25 | train_wall 1786 | gb_free 23.2 | wall 1890
2021-11-20 19:08:33 | INFO | fairseq_cli.train | done training in 1888.7 seconds
lgy-ngram-2
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 19:08:37 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='2', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='2', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 19:08:37 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 19:08:37 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 19:08:37 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 19:08:37 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 19:08:37 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 19:08:42 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 19:08:42 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 19:08:42 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 19:08:42 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 19:08:42 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 19:08:46 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 19:08:46 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 19:08:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:08:46 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 19:08:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:08:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 19:08:46 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 19:08:46 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 19:08:47 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 19:08:47 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 19:08:47 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 19:08:47 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 19:08:47 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 19:08:47 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 19:08:47 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 19:08:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 19:08:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 19:08:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 19:08:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 19:08:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 19:08:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 19:08:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2021-11-20 19:10:00.327829: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2021-11-20 19:10:00.327932: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2021-11-20 19:10:00.327945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-11-20 19:10:01 | INFO | train_inner | epoch 001:    107 / 2763 bleu_loss=3.532, loss=3.532, ppl=11.57, wps=17048.4, ups=1.51, wpb=11294.1, bsz=363.2, num_updates=100, lr=6e-06, gnorm=3.934, clip=100, loss_scale=1, train_wall=71, gb_free=23.2, wall=74
2021-11-20 19:10:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2021-11-20 19:11:06 | INFO | train_inner | epoch 001:    208 / 2763 bleu_loss=1.371, loss=1.371, ppl=2.59, wps=17510, ups=1.54, wpb=11371.9, bsz=372.3, num_updates=200, lr=1.2e-05, gnorm=3.758, clip=100, loss_scale=0.5, train_wall=65, gb_free=23.1, wall=140
2021-11-20 19:12:13 | INFO | train_inner | epoch 001:    308 / 2763 bleu_loss=0.265, loss=0.265, ppl=1.2, wps=17778, ups=1.49, wpb=11911.8, bsz=351.1, num_updates=300, lr=1.8e-05, gnorm=1.333, clip=100, loss_scale=0.5, train_wall=67, gb_free=20.8, wall=207
2021-11-20 19:13:18 | INFO | train_inner | epoch 001:    408 / 2763 bleu_loss=0.184, loss=0.184, ppl=1.14, wps=17780.8, ups=1.52, wpb=11686.1, bsz=351.2, num_updates=400, lr=2.4e-05, gnorm=0.355, clip=99, loss_scale=0.5, train_wall=65, gb_free=21.6, wall=273
2021-11-20 19:14:21 | INFO | train_inner | epoch 001:    508 / 2763 bleu_loss=0.187, loss=0.187, ppl=1.14, wps=18018.4, ups=1.6, wpb=11278.9, bsz=376, num_updates=500, lr=3e-05, gnorm=0.313, clip=96, loss_scale=0.5, train_wall=62, gb_free=15.1, wall=335
2021-11-20 19:15:25 | INFO | train_inner | epoch 001:    608 / 2763 bleu_loss=0.17, loss=0.17, ppl=1.13, wps=18046.1, ups=1.55, wpb=11607.6, bsz=365.5, num_updates=600, lr=2.96842e-05, gnorm=0.587, clip=91, loss_scale=0.5, train_wall=64, gb_free=13.5, wall=400
2021-11-20 19:16:31 | INFO | train_inner | epoch 001:    708 / 2763 bleu_loss=0.165, loss=0.165, ppl=1.12, wps=17478.7, ups=1.52, wpb=11518.3, bsz=356.2, num_updates=700, lr=2.93684e-05, gnorm=0.266, clip=88, loss_scale=0.5, train_wall=66, gb_free=15.6, wall=466
2021-11-20 19:17:38 | INFO | train_inner | epoch 001:    808 / 2763 bleu_loss=0.16, loss=0.16, ppl=1.12, wps=17424.4, ups=1.49, wpb=11698.9, bsz=348.1, num_updates=800, lr=2.90526e-05, gnorm=0.318, clip=84, loss_scale=0.5, train_wall=67, gb_free=24.6, wall=533
2021-11-20 19:18:43 | INFO | train_inner | epoch 001:    908 / 2763 bleu_loss=0.172, loss=0.172, ppl=1.13, wps=17833, ups=1.55, wpb=11536.2, bsz=370.2, num_updates=900, lr=2.87368e-05, gnorm=0.216, clip=82, loss_scale=0.5, train_wall=64, gb_free=16, wall=597
2021-11-20 19:19:49 | INFO | train_inner | epoch 001:   1008 / 2763 bleu_loss=0.162, loss=0.162, ppl=1.12, wps=17335.1, ups=1.52, wpb=11398.4, bsz=357.8, num_updates=1000, lr=2.84211e-05, gnorm=0.221, clip=78, loss_scale=0.5, train_wall=65, gb_free=16.4, wall=663
2021-11-20 19:20:50 | INFO | train_inner | epoch 001:   1108 / 2763 bleu_loss=0.192, loss=0.192, ppl=1.14, wps=17628.9, ups=1.62, wpb=10878.7, bsz=375.9, num_updates=1100, lr=2.81053e-05, gnorm=0.32, clip=78, loss_scale=0.5, train_wall=61, gb_free=16, wall=725
2021-11-20 19:21:54 | INFO | train_inner | epoch 001:   1208 / 2763 bleu_loss=0.172, loss=0.172, ppl=1.13, wps=17724, ups=1.57, wpb=11310.2, bsz=368.9, num_updates=1200, lr=2.77895e-05, gnorm=0.267, clip=82, loss_scale=0.5, train_wall=63, gb_free=15.3, wall=789
2021-11-20 19:22:59 | INFO | train_inner | epoch 001:   1308 / 2763 bleu_loss=0.166, loss=0.166, ppl=1.12, wps=17265.6, ups=1.54, wpb=11186.7, bsz=351.9, num_updates=1300, lr=2.74737e-05, gnorm=0.286, clip=71, loss_scale=0.5, train_wall=64, gb_free=16.5, wall=854
2021-11-20 19:24:03 | INFO | train_inner | epoch 001:   1408 / 2763 bleu_loss=0.152, loss=0.152, ppl=1.11, wps=18437.9, ups=1.56, wpb=11853.8, bsz=362.5, num_updates=1400, lr=2.71579e-05, gnorm=0.183, clip=71, loss_scale=0.5, train_wall=64, gb_free=20.8, wall=918
2021-11-20 19:25:09 | INFO | train_inner | epoch 001:   1508 / 2763 bleu_loss=0.16, loss=0.16, ppl=1.12, wps=17605.5, ups=1.53, wpb=11483.5, bsz=360.4, num_updates=1500, lr=2.68421e-05, gnorm=0.171, clip=76, loss_scale=0.5, train_wall=65, gb_free=21.6, wall=983
2021-11-20 19:26:16 | INFO | train_inner | epoch 001:   1608 / 2763 bleu_loss=0.148, loss=0.148, ppl=1.11, wps=17564.1, ups=1.49, wpb=11790.9, bsz=344.1, num_updates=1600, lr=2.65263e-05, gnorm=0.159, clip=62, loss_scale=0.5, train_wall=67, gb_free=25.9, wall=1050
2021-11-20 19:27:22 | INFO | train_inner | epoch 001:   1708 / 2763 bleu_loss=0.157, loss=0.157, ppl=1.11, wps=17513.2, ups=1.52, wpb=11542.9, bsz=360.4, num_updates=1700, lr=2.62105e-05, gnorm=0.184, clip=73, loss_scale=0.5, train_wall=66, gb_free=19.3, wall=1116
2021-11-20 19:28:28 | INFO | train_inner | epoch 001:   1808 / 2763 bleu_loss=0.154, loss=0.154, ppl=1.11, wps=17743.5, ups=1.5, wpb=11796.2, bsz=354.5, num_updates=1800, lr=2.58947e-05, gnorm=0.196, clip=71, loss_scale=0.5, train_wall=66, gb_free=15.8, wall=1183
2021-11-20 19:29:33 | INFO | train_inner | epoch 001:   1908 / 2763 bleu_loss=0.16, loss=0.16, ppl=1.12, wps=17976.8, ups=1.54, wpb=11687.9, bsz=359, num_updates=1900, lr=2.55789e-05, gnorm=0.217, clip=74, loss_scale=0.5, train_wall=65, gb_free=20.3, wall=1248
2021-11-20 19:30:36 | INFO | train_inner | epoch 001:   2008 / 2763 bleu_loss=0.168, loss=0.168, ppl=1.12, wps=17191.1, ups=1.58, wpb=10881.2, bsz=373.1, num_updates=2000, lr=2.52632e-05, gnorm=0.201, clip=74, loss_scale=0.5, train_wall=63, gb_free=19.2, wall=1311
2021-11-20 19:31:41 | INFO | train_inner | epoch 001:   2108 / 2763 bleu_loss=0.152, loss=0.152, ppl=1.11, wps=17986.5, ups=1.54, wpb=11652, bsz=360.4, num_updates=2100, lr=2.49474e-05, gnorm=0.186, clip=67, loss_scale=0.5, train_wall=64, gb_free=23.2, wall=1376
2021-11-20 19:32:46 | INFO | train_inner | epoch 001:   2208 / 2763 bleu_loss=0.152, loss=0.152, ppl=1.11, wps=17917.3, ups=1.54, wpb=11623, bsz=356.1, num_updates=2200, lr=2.46316e-05, gnorm=0.187, clip=67, loss_scale=0.5, train_wall=65, gb_free=17.1, wall=1441
2021-11-20 19:33:51 | INFO | train_inner | epoch 001:   2308 / 2763 bleu_loss=0.147, loss=0.147, ppl=1.11, wps=17844.3, ups=1.53, wpb=11672, bsz=360.3, num_updates=2300, lr=2.43158e-05, gnorm=0.274, clip=62, loss_scale=0.5, train_wall=65, gb_free=18.2, wall=1506
2021-11-20 19:34:56 | INFO | train_inner | epoch 001:   2408 / 2763 bleu_loss=0.15, loss=0.15, ppl=1.11, wps=17876.2, ups=1.54, wpb=11603.1, bsz=353.2, num_updates=2400, lr=2.4e-05, gnorm=0.173, clip=63, loss_scale=0.5, train_wall=65, gb_free=20, wall=1571
2021-11-20 19:35:58 | INFO | train_inner | epoch 001:   2508 / 2763 bleu_loss=0.159, loss=0.159, ppl=1.12, wps=18489.6, ups=1.62, wpb=11386.8, bsz=380.6, num_updates=2500, lr=2.36842e-05, gnorm=0.22, clip=77, loss_scale=0.5, train_wall=61, gb_free=20.4, wall=1632
2021-11-20 19:37:04 | INFO | train_inner | epoch 001:   2608 / 2763 bleu_loss=0.152, loss=0.152, ppl=1.11, wps=17777.1, ups=1.53, wpb=11649.5, bsz=364.8, num_updates=2600, lr=2.33684e-05, gnorm=0.188, clip=65, loss_scale=0.5, train_wall=65, gb_free=20.4, wall=1698
2021-11-20 19:38:07 | INFO | train_inner | epoch 001:   2708 / 2763 bleu_loss=0.157, loss=0.157, ppl=1.12, wps=18114.5, ups=1.57, wpb=11555.2, bsz=366.7, num_updates=2700, lr=2.30526e-05, gnorm=0.206, clip=65, loss_scale=0.5, train_wall=63, gb_free=24.4, wall=1762
Train_lgy TIME: -1795.103
2021-11-20 19:38:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-11-20 19:40:04 | INFO | valid | epoch 001 | valid on 'valid' subset | bleu_loss 0.019 | loss 0.019 | ppl 1.01 | bleu 0 | wps 1050.6 | wpb 2579.4 | bsz 91 | num_updates 2755
valid_lgy time: 81.304
2021-11-20 19:40:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2755 updates
2021-11-20 19:40:10 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt
2021-11-20 19:40:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf/checkpoint_best.pt (epoch 1 @ 2755 updates, score 0.0) (writing took 11.892342124134302 seconds)
2021-11-20 19:40:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-11-20 19:40:16 | INFO | train | epoch 001 | bleu_loss 0.329 | loss 0.329 | ppl 1.26 | wps 16848.1 | ups 1.46 | wpb 11513.4 | bsz 361.9 | num_updates 2755 | lr 2.28789e-05 | gnorm 0.545 | clip 78.4 | loss_scale 0.5 | train_wall 1784 | gb_free 23.2 | wall 1890
2021-11-20 19:40:16 | INFO | fairseq_cli.train | done training in 1888.3 seconds
lgy-ngram-3
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 19:40:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='3', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='3', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 19:40:20 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 19:40:20 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 19:40:20 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 19:40:20 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 19:40:20 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 19:40:25 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 19:40:25 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 19:40:25 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 19:40:25 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 19:40:25 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 19:40:28 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 19:40:28 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 19:40:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:40:28 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 19:40:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:40:28 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 19:40:28 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 19:40:28 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 19:40:30 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 19:40:30 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 19:40:30 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 19:40:30 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 19:40:30 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 19:40:30 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 19:40:30 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 19:40:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 19:40:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 19:40:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 19:40:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 19:40:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 19:40:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 19:40:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Traceback (most recent call last):
  File "/home/lptang/anaconda3/envs/torch/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 464, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/lptang/fairseq/fairseq/distributed/utils.py", line 361, in call_main
    main(cfg, **kwargs)
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 148, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 250, in train
    log_output = trainer.train_step(samples)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/lptang/fairseq/fairseq/trainer.py", line 586, in train_step
    raise e
  File "/home/lptang/fairseq/fairseq/trainer.py", line 560, in train_step
    ignore_grad=is_dummy_batch,
  File "/home/lptang/fairseq/fairseq/tasks/fairseq_task.py", line 432, in train_step
    optimizer.backward(loss)
  File "/home/lptang/fairseq/fairseq/optim/fp16_optimizer.py", line 101, in backward
    loss.backward()
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
lgy-ngram-3
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 19:41:13 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='3', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='3', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 19:41:13 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 19:41:13 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 19:41:13 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 19:41:13 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 19:41:13 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 19:41:18 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 19:41:18 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 19:41:18 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 19:41:18 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 19:41:18 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 19:41:21 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 19:41:21 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 19:41:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:41:21 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 19:41:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:41:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 19:41:21 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 19:41:21 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 19:41:22 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 19:41:22 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 19:41:23 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 19:41:23 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 19:41:23 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 19:41:23 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 19:41:23 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 19:41:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 19:41:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 19:41:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 19:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 19:41:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 19:41:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 19:41:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Traceback (most recent call last):
  File "/home/lptang/anaconda3/envs/torch/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 464, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/lptang/fairseq/fairseq/distributed/utils.py", line 361, in call_main
    main(cfg, **kwargs)
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 148, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 250, in train
    log_output = trainer.train_step(samples)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/lptang/fairseq/fairseq/trainer.py", line 586, in train_step
    raise e
  File "/home/lptang/fairseq/fairseq/trainer.py", line 560, in train_step
    ignore_grad=is_dummy_batch,
  File "/home/lptang/fairseq/fairseq/tasks/fairseq_task.py", line 432, in train_step
    optimizer.backward(loss)
  File "/home/lptang/fairseq/fairseq/optim/fp16_optimizer.py", line 101, in backward
    loss.backward()
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
lgy-ngram-3
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 19:42:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='3', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='3', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 19:42:06 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 19:42:06 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 19:42:06 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 19:42:06 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 19:42:06 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 19:42:11 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 19:42:11 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 19:42:11 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 19:42:11 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 19:42:11 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 19:42:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 19:42:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 19:42:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:42:15 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 19:42:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:42:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 19:42:15 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 19:42:15 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 19:42:16 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 19:42:16 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 19:42:16 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 19:42:16 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 19:42:16 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 19:42:16 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 19:42:16 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 19:42:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 19:42:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 19:42:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 19:42:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 19:42:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 19:42:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 19:42:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Traceback (most recent call last):
  File "/home/lptang/anaconda3/envs/torch/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 464, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/lptang/fairseq/fairseq/distributed/utils.py", line 361, in call_main
    main(cfg, **kwargs)
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 148, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 250, in train
    log_output = trainer.train_step(samples)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/lptang/fairseq/fairseq/trainer.py", line 586, in train_step
    raise e
  File "/home/lptang/fairseq/fairseq/trainer.py", line 560, in train_step
    ignore_grad=is_dummy_batch,
  File "/home/lptang/fairseq/fairseq/tasks/fairseq_task.py", line 432, in train_step
    optimizer.backward(loss)
  File "/home/lptang/fairseq/fairseq/optim/fp16_optimizer.py", line 101, in backward
    loss.backward()
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
lgy-ngram-3
mkdir: cannot create directory ‘log/tf/denoising_bart_wmt/complexity’: File exists
2021-11-20 19:42:59 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': 'log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 6000, 'batch_size': 128, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 6000, 'batch_size_valid': 128, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', 'restore_file': '/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_base', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='3', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'wmt18-raw-bin/wmt18_raw_raw_1m', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': Namespace(_name='ngrambleuloss_nat', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='bart_base', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='bleu', bestbleu_ngram=0, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='ngrambleuloss_nat', cross_self_attention=False, curriculum=0, data='wmt18-raw-bin/wmt18_raw_raw_1m', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_tokens_valid=6000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, ngram='3', no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt', sample_method='greedy', save_dir='checkpoints/denoising_bart/complexity/ng_0_ba_1.0_cetf', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='log/tf/denoising_bart_wmt/complexity/ng_0_ba_1.0_cetf', tf_ratio='1.0', threshold_loss_scale=None, tokenizer=None, top_k=5, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.01, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2021-11-20 19:42:59 | INFO | fairseq.tasks.translation | [de] dictionary: 51200 types
2021-11-20 19:42:59 | INFO | fairseq.tasks.translation | [en] dictionary: 51200 types
2021-11-20 19:42:59 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.de
2021-11-20 19:42:59 | INFO | fairseq.data.data_utils | loaded 3,003 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/valid.de-en.en
2021-11-20 19:42:59 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m valid de-en 3003 examples
2021-11-20 19:43:04 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51200, 768, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=768, out_features=51200, bias=False)
  )
  (classification_heads): ModuleDict()
)
2021-11-20 19:43:04 | INFO | fairseq_cli.train | task: TranslationTask
2021-11-20 19:43:04 | INFO | fairseq_cli.train | model: BARTModel
2021-11-20 19:43:04 | INFO | fairseq_cli.train | criterion: NgramBLEULossNATCriterion
2021-11-20 19:43:04 | INFO | fairseq_cli.train | num. model params: 140,138,496 (num. trained: 140,138,496)
2021-11-20 19:43:08 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-11-20 19:43:08 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-11-20 19:43:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:43:08 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.717 GB ; name = Tesla V100-DGXS-32GB                    
2021-11-20 19:43:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-11-20 19:43:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-11-20 19:43:08 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and batch size per GPU = 128
2021-11-20 19:43:08 | INFO | fairseq.trainer | Preparing to load checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt
2021-11-20 19:43:09 | INFO | fairseq.trainer | Loaded checkpoint /home/lptang/fairseq/checkpoints/denoising_bart_wmt/wmt18_0904/cr_ba_1.0_raw_1m/checkpoint_best.pt (epoch 3 @ 0 updates)
2021-11-20 19:43:09 | INFO | fairseq.trainer | loading train data for epoch 1
2021-11-20 19:43:09 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.de
2021-11-20 19:43:09 | INFO | fairseq.data.data_utils | loaded 1,000,000 examples from: wmt18-raw-bin/wmt18_raw_raw_1m/train.de-en.en
2021-11-20 19:43:09 | INFO | fairseq.tasks.translation | wmt18-raw-bin/wmt18_raw_raw_1m train de-en 1000000 examples
2021-11-20 19:43:09 | WARNING | fairseq.tasks.fairseq_task | 53 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286537, 478888, 453684, 822029, 11022, 226188, 407738, 943618, 708615, 604516]
2021-11-20 19:43:09 | INFO | fairseq.trainer | begin training epoch 1
/home/lptang/fairseq/fairseq/criterions/bleuloss.py:28: UserWarning: Criterions should take explicit arguments instead of an argparse.Namespace object, please update your criterion by extending FairseqCriterion instead of LegacyFairseqCriterion.
  super().__init__(args,task)
2021-11-20 19:43:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-11-20 19:43:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-11-20 19:43:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-11-20 19:43:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-11-20 19:43:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-11-20 19:43:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-11-20 19:43:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
Traceback (most recent call last):
  File "/home/lptang/anaconda3/envs/torch/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 464, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/lptang/fairseq/fairseq/distributed/utils.py", line 361, in call_main
    main(cfg, **kwargs)
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 148, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/lptang/fairseq/fairseq_cli/train.py", line 250, in train
    log_output = trainer.train_step(samples)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/home/lptang/fairseq/fairseq/trainer.py", line 586, in train_step
    raise e
  File "/home/lptang/fairseq/fairseq/trainer.py", line 560, in train_step
    ignore_grad=is_dummy_batch,
  File "/home/lptang/fairseq/fairseq/tasks/fairseq_task.py", line 432, in train_step
    optimizer.backward(loss)
  File "/home/lptang/fairseq/fairseq/optim/fp16_optimizer.py", line 101, in backward
    loss.backward()
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/lptang/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
